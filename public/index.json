[
{
	"uri": "http://localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: Vietnam Cloud Day 2025 Event Objectives Share real-world experiences from enterprises that have migrated and modernized on-premises workloads with AWS Introduce AI-powered Amazon Q developer to accelerate the SDLC Discuss application modernization strategies that drive digital business transformation Provide guidance on modernizing VMware estates with AI-driven cloud Present approaches to achieving AWS security at scale Speakers Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Hung Hoang - Customer Solutions Manager, AWS Taiki Dang - Solutions Architect, AWS Key Highlights Large-scale migration and modernization with AWS Lessons from thousands of enterprises: migrate and modernize simultaneously Best practices, proven mental models, and AWS migration accelerators Modernizing applications with generative AI (Amazon Q Developer) Q Developer enhances the entire SDLC across AWS console, IDE, CLI, and DevSecOps platforms Automates code generation, documentation to improved quality and maintainability Panel Discussion Experts from OCB, LPBank Securities, and Ninety Eight shared their perspectives Focused on AI-powered modernization, event-driven approaches, and business transformation Emphasis: AI reduces coupling, accelerates development Transforming VMware with AI-driven cloud modernization How AWS transform enables fast, safe, and cost-efficient VMware migrations Step-by-step transformation with human-in-the-loop validation AWS security at scale End-to-end approach: identify, prevent, detect, respond, remediate Security at the edge: layered security Generative AI strengthens security analysis and automates response Key Takeaways Migration and modernization Simply migrating to the cloud is not enough Migration and modernization: go hand-in-hand for sustainable digital transformation AI-powered tools like Q Developer accelerate the SDLC AI can assist in code generation, test creation, documentation, and optimization This improves code quality, shortens development cycles, and reduces operational costs Developers can focus more on innovation instead of repetitive tasks VMware to AWS A step-by-step migration strategy is needed, from lift-and-shift to gradual modernization Downtime-aware migration is critical to avoid business disruption Security Security cannot be an afterthought, it must be integrated throughout the development lifecycle Security by design ensures identity, prevention, detection, and response AI can enhance log analysis, threat detection, and automated remediation Applying to Work Use AI tools like Amazon Q Developer to automate refactoring and testing Embed security from the start with AI-driven detection and response Use AWS Security Hub to centralize findings, prioritize risks, and ensure compliance Event Experience Attending the “Migration \u0026amp; Modernization” track was extremely valuable, giving me a comprehensive perspective on how organizations can migrate, modernize, and secure workloads using AWS services and AI-powered tools. Key experiences included:\nLearning from highly skilled speakers AWS experts and industry leaders shared best practices in migration and modernization at scale Real-world case studies (Techcombank) showed how enterprises build strong cloud foundations and roadmaps for digital transformation Leveraging modern tools Explored Q Developer as an intelligent collaborator across the SDLC to accelerate migration and application refactoring Learned how AI can enhance cloud security by automating analysis, detection, and remediation Saw how AWS Security Hub provides centralized visibility and compliance management across workloads Networking and discussions Had the opportunity to meet new peers who share the same interest and passion for migration and modernization Exchanged insights and experiences with others in the field, fostering mutual learning and collaboration Lessons learned Migration and modernization must go hand-in-hand to achieve sustainable digital transformation Event-driven design reduces coupling while improving scalability and resilience AI tools like Q Developer can cut costs and boost productivity when integrated into workflows Security must be embedded from the start, supported by services like AWS Security Hub for compliance and risk management Event moments Overall, the event enriched my technical expertise while inspiring a new perspective on application design, modernization strategies, and effective cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Student Information: Full Name: Nguyễn Thành Nguyên\nPhone Number: 0935457152\nEmail: nguyen.fridayed@gmail.com\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Build new connections and explore course materials. Understand AWS and its advantages over traditional models. Create and secure an AWS account with MFA and IAM users. Learn cost management and practice creating budgets. Gain hands-on experience with IAM, VPC, and VPN setup and troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Make new friends and connections - Explore course materials and know what to do 09/08/2025 09/08/2025 3 - Learn what AWS is and its advantages over traditional models + Cost + Scalability and elasticity + Agility and speed + Reliability and availability 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create an AWS account - Practice: + Create an account and complete tasks to receive free credits + Enable MFA for stronger security + Create IAM users for better account protection 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about cost management + Learn the 4 types of budgets and their differences - Practice: + Create each type of budget 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about IAM groups and IAM users - Learn how to use roles and their benefits - Learn about firewalls in VPC - How to create a VPC and related components - Site-to-site VPN connection - Practice: + Create an IAM group + Create an IAM user and add it to the group + Create a role and assign it to the user + Create VPC, subnets, and an internet gateway + Create route tables, NAT gateway, and security groups + Create virtual private gateway, customer gateway, and VPN connection + VPN troubleshooting 09/11/2025 09/13/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Built new connections and explored the course materials to understand the learning path.\nUnderstood what AWS is and its key advantages over traditional models:\nCost Scalability and elasticity Agility and speed Reliability and availability \u0026hellip; Successfully created and configured an AWS free tier account, enabled MFA for stronger security, and created IAM users for better account protection.\nLearned cost management concepts and practiced creating the four types of budgets.\nLearned about IAM users, groups, and roles, along with their benefits.\nGained knowledge of VPC, its components, and firewalls in the networking environment.\nPracticed hands-on by creating and configuring:\nIAM groups, users, and roles VPC, subnets, and internet gateway Route tables, NAT gateway, and security groups Virtual private gateway, customer gateway, and VPN connection \u0026hellip; Performed VPN troubleshooting and validated connectivity.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Exploring AWS networking, multi-VPC connectivity, and application deployment\nWeek 3: Practicing AWS core services for monitoring, scaling, and resource management\nWeek 4: Enhancing skills in deploying and managing core AWS services such as DynamoDB, CloudFront, and WorkSpaces\nWeek 5: Strengthening AWS security and governance through IAM, KMS, and CloudTrail\nWeek 6: Improving skills in data migration, security automation, and EC2 management using DMS, Macie, GuardDuty, Secrets Manager, and Lambda\nWeek 7: Exploring AWS Monitoring, Automation, and Infrastructure as Code\nWeek 8: Reinforcing AWS architecture, learning Docker, and building CI/CD pipelines\nWeek 9: Deploying containers on ECS, exploring networking modes, and implementing ALB routing\nWeek 10: Strengthening DevOps with CodeBuild, CodePipeline, ECR, ECS, and building APIs with API Gateway and Lambda\nWeek 11: Containerizing Node.js app, deploying with Docker Compose and practicing IaC with Terraform\nWeek 12: Complete the remaining parts of the final project and learn basic Kubernetes\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "FastAPI Application Structure",
	"tags": [],
	"description": "",
	"content": "FastAPI Application Structure In this section, you will understand the FastAPI application structure and how it\u0026rsquo;s organized using clean layered architecture.\nApplication Architecture The FastAPI application follows a layered architecture pattern:\n┌─────────────────────────────────────────┐ │ API Layer (routers/) │ ← HTTP endpoints, request/response ├─────────────────────────────────────────┤ │ Service Layer (services/) │ ← Business logic, orchestration ├─────────────────────────────────────────┤ │ Repository Layer (repositories/) │ ← Data access, DynamoDB operations ├─────────────────────────────────────────┤ │ Models (models/) │ ← Data structures, validation └─────────────────────────────────────────┘ Main Application Entry Point # backend/app/main.py from fastapi import FastAPI from app.api.routers.auth import router as auth_router from app.api.routers.products import router as products_router from app.api.routers.orders import router as orders_router from app.core.logging import configure_logging from app.api.middleware import add_correlation_middleware def create_app() -\u0026gt; FastAPI: configure_logging() app = FastAPI(title=\u0026#34;Products \u0026amp; Orders API\u0026#34;, version=\u0026#34;1.0.0\u0026#34;) add_correlation_middleware(app) @app.get(\u0026#34;/health\u0026#34;, tags=[\u0026#34;health\u0026#34;]) async def health() -\u0026gt; dict: return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} app.include_router(auth_router, prefix=\u0026#34;/auth\u0026#34;, tags=[\u0026#34;auth\u0026#34;]) app.include_router(products_router, prefix=\u0026#34;/products\u0026#34;, tags=[\u0026#34;products\u0026#34;]) app.include_router(orders_router, prefix=\u0026#34;/orders\u0026#34;, tags=[\u0026#34;orders\u0026#34;]) return app app = create_app() Lambda Handler # backend/app/lambda_handler.py from mangum import Mangum from app.main import app # Expose Lambda handler for API Gateway HTTP API handler = Mangum(app) How Mangum Works:\nConverts API Gateway HTTP API events to ASGI requests Routes requests to FastAPI application Converts FastAPI responses back to API Gateway format Configuration Management # backend/app/core/config.py from pydantic_settings import BaseSettings from pydantic import Field import os class Settings(BaseSettings): aws_region: str = Field(..., alias=\u0026#34;AWS_REGION\u0026#34;) products_table: str = Field(..., alias=\u0026#34;PRODUCTS_TABLE\u0026#34;) orders_table: str = Field(..., alias=\u0026#34;ORDERS_TABLE\u0026#34;) jwt_secret: str = Field(..., alias=\u0026#34;JWT_SECRET\u0026#34;) log_level: str = Field(\u0026#34;INFO\u0026#34;, alias=\u0026#34;LOG_LEVEL\u0026#34;) class Config: env_file = os.getenv(\u0026#34;ENV_FILE\u0026#34;, \u0026#34;.env\u0026#34;) case_sensitive = True settings = Settings() # will raise if required env missing Environment Variables:\nAWS_REGION: AWS region (e.g., us-east-1) PRODUCTS_TABLE: DynamoDB products table name ORDERS_TABLE: DynamoDB orders table name JWT_SECRET: JWT secret from Secrets Manager LOG_LEVEL: Logging level (default: INFO) API Routers Auth Router # backend/app/api/routers/auth.py from fastapi import APIRouter, Depends, HTTPException, status from pydantic import BaseModel, EmailStr from app.api.deps import get_auth_service from app.services.auth_service import AuthService router = APIRouter() class RegisterRequest(BaseModel): email: EmailStr password: str class LoginRequest(BaseModel): email: EmailStr password: str class TokenResponse(BaseModel): access_token: str token_type: str = \u0026#34;bearer\u0026#34; @router.post(\u0026#34;/register\u0026#34;, status_code=status.HTTP_201_CREATED) async def register(payload: RegisterRequest, auth_service: AuthService = Depends(get_auth_service)) -\u0026gt; dict: await auth_service.register_user(email=payload.email, password=payload.password) return {\u0026#34;message\u0026#34;: \u0026#34;registered\u0026#34;} @router.post(\u0026#34;/login\u0026#34;, response_model=TokenResponse) async def login(payload: LoginRequest, auth_service: AuthService = Depends(get_auth_service)) -\u0026gt; TokenResponse: token = await auth_service.login(email=payload.email, password=payload.password) return TokenResponse(access_token=token) Endpoints:\nPOST /auth/register - Register new user POST /auth/login - Login and get JWT token Products Router # backend/app/api/routers/products.py from fastapi import APIRouter, Depends, HTTPException, status, Query from typing import Optional, List from app.api.deps import require_admin, get_product_service from app.services.product_service import ProductService from app.models.product import ProductCreate, ProductUpdate, ProductOut router = APIRouter() @router.post(\u0026#34;\u0026#34;, response_model=ProductOut, status_code=status.HTTP_201_CREATED, dependencies=[Depends(require_admin)]) async def create_product(payload: ProductCreate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.create_product(payload) @router.get(\u0026#34;\u0026#34;, response_model=List[ProductOut]) async def list_products(category: Optional[str] = Query(default=None), service: ProductService = Depends(get_product_service)) -\u0026gt; List[ProductOut]: return await service.list_products(category=category) @router.get(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut) async def get_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: product = await service.get_product(product_id) if not product: raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\u0026#34;Product not found\u0026#34;) return product @router.put(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut, dependencies=[Depends(require_admin)]) async def update_product(product_id: str, payload: ProductUpdate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.update_product(product_id, payload) @router.delete(\u0026#34;/{product_id}\u0026#34;, status_code=status.HTTP_204_NO_CONTENT, dependencies=[Depends(require_admin)]) async def delete_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; None: await service.delete_product(product_id) Endpoints:\nGET /products - List all products (optional category filter) GET /products/{product_id} - Get single product POST /products - Create product (admin only) PUT /products/{product_id} - Update product (admin only) DELETE /products/{product_id} - Delete product (admin only) Dependency Injection # backend/app/api/deps.py from fastapi import Depends, HTTPException, status from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer from typing import Optional from app.core.security import decode_jwt from app.core.config import settings from app.models.user import CurrentUser, Role from app.services.product_service import ProductService from app.repositories.ddb_products import DynamoProductsRepository security = HTTPBearer(auto_error=False) def get_products_repo() -\u0026gt; DynamoProductsRepository: return DynamoProductsRepository(settings.aws_region, settings.products_table) def get_product_service(repo: DynamoProductsRepository = Depends(get_products_repo)) -\u0026gt; ProductService: return ProductService(repo) def get_current_user(creds: Optional[HTTPAuthorizationCredentials] = Depends(security)) -\u0026gt; CurrentUser: if creds is None: raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Missing credentials\u0026#34;) payload = decode_jwt(creds.credentials) sub = payload.get(\u0026#34;sub\u0026#34;) role = payload.get(\u0026#34;role\u0026#34;, Role.user.value) if not sub: raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Invalid token\u0026#34;) return CurrentUser(user_id=sub, role=Role(role)) def require_admin(role: Role = Depends(get_current_user_role)) -\u0026gt; None: if role != Role.admin: raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\u0026#34;Admin only\u0026#34;) JWT Security Implementation # backend/app/core/security.py from datetime import datetime, timedelta, timezone from typing import Dict, Any from jose import jwt from passlib.context import CryptContext from app.core.config import settings ALGORITHM = \u0026#34;HS256\u0026#34; ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 6 # 6 hours pwd_context = CryptContext(schemes=[\u0026#34;bcrypt\u0026#34;], deprecated=\u0026#34;auto\u0026#34;) def create_access_token(subject: str, role: str) -\u0026gt; str: expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES) to_encode: Dict[str, Any] = {\u0026#34;sub\u0026#34;: subject, \u0026#34;role\u0026#34;: role, \u0026#34;exp\u0026#34;: expire} return jwt.encode(to_encode, settings.jwt_secret, algorithm=ALGORITHM) def decode_jwt(token: str) -\u0026gt; Dict[str, Any]: return jwt.decode(token, settings.jwt_secret, algorithms=[ALGORITHM]) def verify_password(plain_password: str, hashed_password: str) -\u0026gt; bool: return pwd_context.verify(plain_password, hashed_password) def get_password_hash(password: str) -\u0026gt; str: return pwd_context.hash(password) Security Features:\nJWT tokens with 6-hour expiration Password hashing using bcrypt Role-based access control (admin/user) Token validation on every protected endpoint Correlation ID Middleware # backend/app/api/middleware.py import uuid import logging from starlette.middleware.base import BaseHTTPMiddleware from starlette.requests import Request logger = logging.getLogger(__name__) class CorrelationMiddleware(BaseHTTPMiddleware): async def dispatch(self, request: Request, call_next): correlation_id = request.headers.get(\u0026#34;X-Correlation-ID\u0026#34;, str(uuid.uuid4())) logger.info(f\u0026#34;Request started\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;method\u0026#34;: request.method, \u0026#34;path\u0026#34;: request.url.path }) response = await call_next(request) response.headers[\u0026#34;X-Correlation-ID\u0026#34;] = correlation_id logger.info(f\u0026#34;Request completed\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;status_code\u0026#34;: response.status_code }) return response def add_correlation_middleware(app): app.add_middleware(CorrelationMiddleware) Purpose:\nTrack requests across logs using correlation IDs Useful for debugging and tracing requests in distributed systems Dockerfile # backend/Dockerfile # Lambda base image for Python 3.10 FROM public.ecr.aws/lambda/python:3.10 # Copy and install requirements COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt -t \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; # Copy application code COPY app/ ${LAMBDA_TASK_ROOT}/app/ # Set the handler CMD [\u0026#34;app.lambda_handler.handler\u0026#34;] Key Points:\nUses official AWS Lambda Python 3.10 base image Installs dependencies to ${LAMBDA_TASK_ROOT} Copies application code Sets handler to app.lambda_handler.handler Local Development To run locally:\n# Install dependencies make install # Run with uvicorn make run-local # or uvicorn backend.app.main:app --reload Access Swagger documentation at: http://localhost:8000/docs\nThe layered architecture (API → Service → Repository) makes the code testable, maintainable, and follows separation of concerns principles. Each layer has a single responsibility.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Quick Start: DevSecOps with FastAPI Backend",
	"tags": [],
	"description": "",
	"content": "Quick Start: DevSecOps with FastAPI Backend This workshop guides you through quickly setting up a DevSecOps pipeline for a FastAPI Backend on AWS Lambda. Just configure a few basic things and the pipeline will automatically build, scan, and deploy.\nRepository: https://gitlab.com/m.quang/devsecops-aws-ver2\nQuick Start Checklist Just follow these 5 steps:\nConfigure AWS CLI - aws configure Create JWT Secret - aws secretsmanager create-secret Create ECR Repository - aws ecr create-repository Configure GitLab Variables - Add 7 variables in GitLab Push Code - git push and the pipeline will automatically run Time: ~15-20 minutes\nWhat You\u0026rsquo;ll Get? After completion, you\u0026rsquo;ll have:\nFastAPI Backend running on AWS Lambda (container) GitLab CI/CD Pipeline automatically: Semgrep - Security scan (SAST) Docker Build - Build container image Trivy - Vulnerability scan Terraform Deploy - Automatically deploy infrastructure API Gateway - REST API endpoint DynamoDB - 3 tables (products, orders, users) CloudWatch - Monitoring \u0026amp; alerting Detailed Steps Step 1: Clone \u0026amp; Configure AWS CLI # Clone repository git clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline # Configure AWS CLI aws configure # Enter: Access Key, Secret Key, Region (ap-southeast-1), Format (json) Step 2: Create JWT Secret JWT_SECRET=$(openssl rand -hex 32) aws secretsmanager create-secret \\ --name fastapi-jwt-secret \\ --secret-string \u0026#34;$JWT_SECRET\u0026#34; \\ --region ap-southeast-1 # Get ARN (save it) JWT_SECRET_ARN=$(aws secretsmanager describe-secret \\ --secret-id fastapi-jwt-secret \\ --region ap-southeast-1 \\ --query \u0026#39;ARN\u0026#39; \\ --output text) echo \u0026#34;JWT_SECRET_ARN: $JWT_SECRET_ARN\u0026#34; Step 3: Create ECR Repository aws ecr create-repository \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 # Get ECR URI (save it) ECR_URI=$(aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].repositoryUri\u0026#39; \\ --output text) echo \u0026#34;ECR_URI: $ECR_URI\u0026#34; Step 4: Configure GitLab CI/CD Variables Go to GitLab project → Settings → CI/CD → Variables → Expand\nAdd 7 variables:\nVariable Value Protected Masked AWS_ACCESS_KEY_ID Your AWS Access Key ✅ ❌ AWS_SECRET_ACCESS_KEY Your AWS Secret Key ✅ ✅ AWS_DEFAULT_REGION ap-southeast-1 ❌ ❌ JWT_SECRET_ARN arn:aws:secretsmanager:... ❌ ❌ ECR_URI 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda ❌ ❌ PROJECT_NAME fastapi-lambda ❌ ❌ LAMBDA_FUNCTION_NAME fastapi-lambda-fn ❌ ❌ Step 5: Setup GitLab CI/CD \u0026amp; Push # Copy GitLab CI file (from repository root) cd .. cp Backend-FastAPI-Docker_Build-Pipeline/.gitlab-ci.yml.example .gitlab-ci.yml # Commit and push git add .gitlab-ci.yml git commit -m \u0026#34;Add GitLab CI/CD pipeline\u0026#34; git push origin main Pipeline automatically runs:\nlint_and_scan - Semgrep security scan build_and_push - Docker build + Trivy scan + ECR push terraform_deploy - Deploy infrastructure Architecture Overview Detailed Sections The workshop is divided into sections:\nPrerequisites - Setup AWS CLI, GitLab account Setup GitLab CI/CD - Configure pipeline and variables Security Scanning - Semgrep \u0026amp; Trivy Deploy Backend - Infrastructure automatically deployed Tech Stack Component Technology Backend FastAPI (Python) Runtime AWS Lambda (Container) Database DynamoDB CI/CD GitLab CI/CD Security Semgrep (SAST), Trivy (Container Scan) Infrastructure Terraform Monitoring CloudWatch, SNS Verification After the pipeline finishes running:\n# Get API URL from GitLab job logs or: cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url # Test API curl https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Cost Estimate Lambda: ~$1-3/month (250K requests) DynamoDB: ~$5-10/month (on-demand) API Gateway: ~$1-3/month ECR: ~$0.50-1.50/month Total: ~$10-20/month (low traffic) Next Steps Read Prerequisites to setup the environment Follow Setup GitLab CI/CD to configure the pipeline View Security Scanning to understand how scanning works Check Deploy Backend to see the infrastructure being created This workshop is based on a real project folder Backend-FastAPI-Docker_Build-Pipeline. All code and configuration are ready, you just need to configure and run!\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Setup GitLab CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Setup GitLab CI/CD Pipeline This section guides you through setting up the GitLab CI/CD pipeline. The pipeline automatically builds, scans, and deploys your FastAPI application using Terraform.\nStep 1: Clone the Repository Clone the project repository:\ngit clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\nStep 2: Configure AWS CLI Configure AWS CLI with your credentials:\naws configure Enter the following when prompted:\nAWS Access Key ID: Your IAM user access key AWS Secret Access Key: Your IAM user secret key Default region name: ap-southeast-1 (or your preferred region) Default output format: json Verify the configuration:\naws sts get-caller-identity You should see your AWS account ID and user ARN.\nStep 3: Create JWT Secret in AWS Secrets Manager Create a JWT secret for authentication:\n# Generate a random secret (or use your own) JWT_SECRET=$(openssl rand -hex 32) # Create secret in Secrets Manager aws secretsmanager create-secret \\ --name fastapi-jwt-secret \\ --secret-string \u0026#34;$JWT_SECRET\u0026#34; \\ --region ap-southeast-1 # Get the secret ARN (save this for GitLab variables) aws secretsmanager describe-secret \\ --secret-id fastapi-jwt-secret \\ --region ap-southeast-1 \\ --query \u0026#39;ARN\u0026#39; \\ --output text Note the Secret ARN - you\u0026rsquo;ll need it for GitLab variables (e.g., arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:fastapi-jwt-secret-xxxxx)\nStep 4: Configure GitLab CI/CD Variables Navigate to your GitLab project and configure CI/CD variables:\nGo to your GitLab project: https://gitlab.com/YOUR_USERNAME/devsecops-aws-ver2 Navigate to Settings → CI/CD → Variables Click Expand on \u0026ldquo;Variables\u0026rdquo; section Add the following variables: Variable Key Value Protected Masked Description AWS_ACCESS_KEY_ID Your AWS Access Key ✅ ❌ AWS access key for CI/CD AWS_SECRET_ACCESS_KEY Your AWS Secret Key ✅ ✅ AWS secret key (masked) AWS_DEFAULT_REGION ap-southeast-1 ❌ ❌ AWS region JWT_SECRET_ARN arn:aws:secretsmanager:... ❌ ❌ Secrets Manager ARN for JWT ECR_REPO_NAME fastapi-lambda ❌ ❌ ECR repository name LAMBDA_FUNCTION_NAME fastapi-lambda-fn ❌ ❌ Lambda function name PROJECT_NAME fastapi-lambda ❌ ❌ Project name for resources Important:\nMark AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as Protected (only available in protected branches) Mark AWS_SECRET_ACCESS_KEY as Masked (hidden in logs) Use Expand button to add each variable Step 5: Create GitLab CI/CD Pipeline File The project uses GitLab CI/CD with three stages. Create or verify .gitlab-ci.yml in the root of your repository:\nstages: - lint - build - deploy variables: IMAGE_NAME: \u0026#34;fastapi-lambda\u0026#34; IMAGE_TAG: \u0026#34;${CI_COMMIT_SHORT_SHA}\u0026#34; lint_and_scan: stage: lint image: docker:24.0.7 services: - docker:24.0.7-dind before_script: - apk add --no-cache python3 py3-pip bash curl - pip3 install semgrep script: - echo \u0026#34;Running Semgrep SAST scan...\u0026#34; - semgrep --config Backend-FastAPI-Docker_Build-Pipeline/backend/semgrep.yml Backend-FastAPI-Docker_Build-Pipeline/backend/ artifacts: reports: codequality: semgrep-report.json paths: - semgrep-report.json expire_in: 1 week only: - main - merge_requests build_and_push: stage: build image: docker:24.0.7 services: - docker:24.0.7-dind before_script: - apk add --no-cache python3 py3-pip bash curl jq aws-cli - curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin - aws --version - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ECR_URI script: - echo \u0026#34;Building Docker image...\u0026#34; - cd Backend-FastAPI-Docker_Build-Pipeline - docker build -t $IMAGE_NAME:$IMAGE_TAG backend - echo \u0026#34;Scanning filesystem with Trivy...\u0026#34; - trivy fs --exit-code 1 --severity HIGH,CRITICAL --ignorefile backend/trivyignore.txt . - echo \u0026#34;Scanning image with Trivy...\u0026#34; - trivy image --exit-code 1 --severity HIGH,CRITICAL $IMAGE_NAME:$IMAGE_TAG - echo \u0026#34;Tagging and pushing to ECR...\u0026#34; - docker tag $IMAGE_NAME:$IMAGE_TAG $ECR_URI:$IMAGE_TAG - docker push $ECR_URI:$IMAGE_TAG - echo \u0026#34;$ECR_URI:$IMAGE_TAG\u0026#34; \u0026gt; image-uri.txt artifacts: paths: - Backend-FastAPI-Docker_Build-Pipeline/image-uri.txt expire_in: 1 week only: - main terraform_deploy: stage: deploy image: hashicorp/terraform:1.9.5 before_script: - apk add --no-cache aws-cli - aws --version - cd Backend-FastAPI-Docker_Build-Pipeline script: - echo \u0026#34;Initializing Terraform...\u0026#34; - cd infra - terraform init -upgrade - echo \u0026#34;Planning Terraform changes...\u0026#34; - terraform plan -input=false -out=tfplan \\ -var \u0026#34;project_name=$PROJECT_NAME\u0026#34; \\ -var \u0026#34;region=$AWS_DEFAULT_REGION\u0026#34; \\ -var \u0026#34;jwt_secret_arn=$JWT_SECRET_ARN\u0026#34; \\ -var \u0026#34;image_uri=$ECR_URI:$IMAGE_TAG\u0026#34; - echo \u0026#34;Applying Terraform changes...\u0026#34; - terraform apply -auto-approve tfplan - echo \u0026#34;Deployment completed successfully!\u0026#34; dependencies: - build_and_push only: - main Note: Replace $ECR_URI with your actual ECR repository URI. You\u0026rsquo;ll get this after creating the ECR repository.\nStep 6: Create ECR Repository Create the ECR repository for storing Docker images:\n# Create ECR repository aws ecr create-repository \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --encryption-configuration encryptionType=AES256 \\ --region ap-southeast-1 # Get ECR repository URI ECR_URI=$(aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].repositoryUri\u0026#39; \\ --output text) echo \u0026#34;ECR URI: $ECR_URI\u0026#34; Add ECR_URI to GitLab CI/CD variables:\nVariable Key: ECR_URI Value: 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda (your actual URI) Step 7: Push Code and Trigger Pipeline Commit and push your code to trigger the pipeline:\n# Add .gitlab-ci.yml if you created it git add .gitlab-ci.yml git commit -m \u0026#34;Add GitLab CI/CD pipeline\u0026#34; git push origin main The pipeline will automatically:\nLint Stage: Run Semgrep SAST scan Build Stage: Build Docker image, scan with Trivy, push to ECR Deploy Stage: Run Terraform to deploy infrastructure Step 8: Monitor Pipeline Execution Go to your GitLab project Navigate to CI/CD → Pipelines Click on the running pipeline to view job logs Monitor each stage: lint_and_scan: Semgrep security scan build_and_push: Docker build and Trivy scan terraform_deploy: Infrastructure deployment After the pipeline completes successfully, you should see all stages with green checkmarks:\nStep 9: Get API Gateway URL After successful deployment, get the API Gateway URL:\ncd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url Or from GitLab CI/CD job logs, look for Terraform output.\nTroubleshooting Pipeline Fails at Lint Stage Check Semgrep findings in job logs Fix security issues in code Update backend/semgrep.yml if needed Pipeline Fails at Build Stage Verify ECR_URI variable is correct Check AWS credentials have ECR permissions Verify Docker image builds locally: docker build -t test backend Pipeline Fails at Deploy Stage Check Terraform logs in job output Verify all GitLab variables are set correctly Ensure IAM user has permissions for Lambda, API Gateway, DynamoDB AWS Credentials Issues Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are correct Check IAM user has required permissions Test credentials: aws sts get-caller-identity The pipeline automatically triggers on every push to the main branch. You can also manually trigger it from GitLab CI/CD → Pipelines → Run Pipeline.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn how to manage and connect EC2 instances using session manager. Practice VPC peering and apply ACL and security group rules for cross-VPC communication. Understand the difference between VPC peering and transit gateway. Explore hybrid DNS with route 53 resolver and AWS managed microsoft AD. Deploy applications on EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about session manager - Practice: + Create VPC endpoint for session manager + Create S3 bucket to store session logs 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about VPC peering - How to apply rules in ACLs and security groups for cross-VPC traffic - Practice: + Create 2 VPCs + Set up VPC peering + Apply rules in ACLs and security groups for cross-VPC traffic 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about transit gateway - Understand the different between VPC peering and transit gateway - Practice: + Create 4 VPCs + Create transit gateway and attachments + Configure transit gateway route table and VPC route tables 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about hybrid DNS with route 53 resolver - Learn about AWS directory service and AWS managed microsoft AD - Practice: + Set up network infrastructure using AWS cloudFormation + Configure inbound/outbound endpoints for DNS resolution 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about application deployment on EC2 (Windows and Linux) - Practice: + Deploying databases and web services via CLI on linux + Deploying databases and web services via GUI and CLI on windows 09/18/2025 09/20/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Successfully managed and connected to EC2 instances in both public and private subnets using AWS session manager.\nEstablished VPC peering between two VPCs and applied proper ACL and security group rules to enable cross-VPC communication.\nValidated and troubleshooted VPC peering connectivity using reachability analyzer to identify and fix route table and security group issues.\nBuilt multi-VPC connectivity with AWS transit gateway, including:\nTransit gateway attachments Configured transit gateway route table Configured VPC route table \u0026hellip; Implemented Hybrid DNS resolution using Route 53 Resolver.\nSet up AWS Managed Microsoft AD through AWS Directory Service and deployed network infrastructure using CloudFormation.\nDeployed applications on EC2 instances (Windows and Linux) using both CLI and GUI methods.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: AI-Driven Development Life Cycle: Reimagining Software Engineering Event Objectives The role of AI in modern software development, from AI-assisted to AI-managed workflows Explore how AI is transforming the development lifecycle with a human-centric approach Identify challenges and opportunities when integrating AI into end-to-end development processes Learn how AI-DLC can accelerate delivery, improve quality, and respond faster to market needs Speakers Toan Huynh - Senior, AWS My Nguyen - Senior, AWS Key Highlights AI shaping software development AI helps developers write code faster and handle multiple tasks simultaneously Current AI can complete end-to-end development tasks, but human-in-the-loop is necessary for validation and control Impact: reduces time from idea to market and improves responsiveness to quality requirements Challenges with AI in development Scaling large projects remains challenging: limited control and potential code quality issues Multi-step tasks require strategic context refresh and careful planning Current AI models: AI-assisted (tasks divided into small units) vs AI-managed (still emerging) AI-Driven lifecycle (AI-DLC) AI acts as a collaborator, not the sole decision-maker Core workflow: plan, seek clarification, provide clarification, implement Lifecycle stages: inception, construction, operation Spec-driven development ensures clear input/output, context, and documentation at every stage Kiro Agent hooks and advanced context management help AI understand complex tasks How to use Kiro to optimize work performance Key Takeaways Human-Centric AI collaboration AI accelerates tasks but does not replace humans, human validation is essential Checkpoints ensure AI output meets expectations and quality standards Structured AI workflow Clear input/output, task division, and context management improve efficiency Interactive approach: AI asks clarifying questions, humans provide guidance Efficiency and quality Faster delivery, reduced coding errors, and better alignment with market requirements Spec-driven development provides process control and ensures consistency Scalability Challenges Managing context and multi-step problems is critical in large-scale projects AI-DLC provides a structured methodology that allows scalable development while preserving control and quality standards Applying to Work Use AI to assist with coding, testing, and deployment for faster delivery Implement AI-DLC workflow for large projects to maintain quality and control Integrate human-in-the-loop validation before deploying AI outputs Apply spec-driven development to ensure clarity of inputs, outputs, and documentation Event Experience Attending the AI-Driven Development Lifecycle event was extremely valuable, providing me with a comprehensive understanding of leveraging AI to accelerate development, optimize workflows, and maintain quality throughout the software lifecycle. Key experiences included:\nLearning from highly skilled speakers Observed how software seniors integrate AI into real development workflows. Learned how human-in-the-loop ensures output quality and control. Understood challenges in scaling projects and managing context effectively. Recognized the importance of documentation and spec-driven development in practical settings. Observing AI Task Demonstrations Saw demos of AI assisting in code generation, testing, and deployment. Experienced AI asking for clarification and humans providing feedback to improve outcomes. Lessons learned AI is a collaborator, not a replacement; human-centric workflow is critical. Clear process, task division, and context management improve efficiency and reduce errors. Interactive workflow ensures continuous improvement and high-quality output. Practical Insights for Future Work AI-DLC methodology can be applied to personal or team projects to improve speed and quality. Experience highlights the need for a structured framework to integrate AI in development. Understanding AI’s potential in large-scale projects with proper management framework. Event moments Overall, the event not only provided practical insights into AI-driven development but also helped me rethink software design, end-to-end development processes, and effective human-AI collaboration.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Deploy Infrastructure",
	"tags": [],
	"description": "",
	"content": "Deploy Infrastructure The infrastructure is automatically deployed by the GitLab CI/CD pipeline. The terraform_deploy job runs Terraform to create all AWS resources.\nAutomatic Deployment via Pipeline When you push code to the main branch, the pipeline automatically:\nLint Stage: Runs Semgrep security scan Build Stage: Builds Docker image, scans with Trivy, pushes to ECR Deploy Stage: Runs Terraform to deploy infrastructure The terraform_deploy job uses the GitLab CI/CD variables you configured earlier.\nInfrastructure Created The Terraform deployment creates:\nDynamoDB Tables:\nproducts - Product catalog orders - Order information users - User accounts and authentication Lambda Function:\nContainer-based Lambda using image from ECR Environment variables from Secrets Manager IAM role with least privilege permissions API Gateway HTTP API:\nRoutes all requests to Lambda function Automatic deployment on changes IAM Roles:\nLambda execution role Permissions for DynamoDB, Secrets Manager, CloudWatch CloudWatch Resources:\nLog groups for Lambda Metric alarms for errors SNS topic for alerts Terraform Module Structure The infrastructure is organized into reusable modules:\ninfra/ ├── main.tf # Orchestrates all modules ├── variables.tf # Input variables ├── outputs.tf # Output values ├── providers.tf # Provider configuration └── modules/ ├── dynamodb/ # DynamoDB tables ├── iam/ # IAM roles and policies ├── lambda_container/ # Lambda function ├── apigw/ # API Gateway HTTP API └── observability/ # CloudWatch and SNS DynamoDB Tables The DynamoDB module creates three tables:\n1. Products Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;products\u0026#34; { name = var.products_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;product_id\u0026#34; attribute { name = \u0026#34;product_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;category\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;category-index\u0026#34; hash_key = \u0026#34;category\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\nproduct_id (String) - Primary key name (String) - Product name price (Number) - Product price stock (Number) - Stock quantity category (String) - Product category updated_at (String) - Last update timestamp Global Secondary Index:\ncategory-index - Query products by category 2. Orders Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;orders\u0026#34; { name = var.orders_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;order_id\u0026#34; attribute { name = \u0026#34;order_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;user-index\u0026#34; hash_key = \u0026#34;user_id\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\norder_id (String) - Primary key user_id (String) - User who placed the order items (List) - Order items total_amount (Number) - Total order amount status (String) - Order status created_at (String) - Order creation timestamp Global Secondary Index:\nuser-index - Query orders by user 3. Users Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;users\u0026#34; { name = var.users_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;user_id\u0026#34; attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;email\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;email-index\u0026#34; hash_key = \u0026#34;email\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\nuser_id (String) - Primary key email (String) - User email (unique) password_hash (String) - Hashed password role (String) - User role (admin/user) created_at (String) - Account creation timestamp Global Secondary Index:\nemail-index - Query users by email Lambda Function The Lambda module creates a container-based function:\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;this\u0026#34; { function_name = \u0026#34;${var.project_name}-fn\u0026#34; package_type = \u0026#34;Image\u0026#34; image_uri = var.image_uri role = var.lambda_role_arn timeout = 15 memory_size = 512 environment { variables = var.environment } } Configuration:\nPackage Type: Container Image Image URI: From ECR (set via GitLab variable ECR_URI) Timeout: 15 seconds Memory: 512 MB Environment Variables: AWS_REGION PRODUCTS_TABLE ORDERS_TABLE JWT_SECRET (fetched from Secrets Manager) API Gateway HTTP API The API Gateway module creates an HTTP API:\nresource \u0026#34;aws_apigatewayv2_api\u0026#34; \u0026#34;http\u0026#34; { name = \u0026#34;${var.project_name}-http\u0026#34; protocol_type = \u0026#34;HTTP\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;lambda\u0026#34; { api_id = aws_apigatewayv2_api.http.id integration_type = \u0026#34;AWS_PROXY\u0026#34; integration_uri = var.lambda_arn } resource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;any\u0026#34; { api_id = aws_apigatewayv2_api.http.id route_key = \u0026#34;$default\u0026#34; target = \u0026#34;integrations/${aws_apigatewayv2_integration.lambda.id}\u0026#34; } Features:\nHTTP API (v2) - Lower latency and cost $default route - All requests go to Lambda Automatic deployment on changes IAM Roles The IAM module creates a Lambda execution role:\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda\u0026#34; { name = \u0026#34;${var.project_name}-lambda-role\u0026#34; assume_role_policy = data.aws_iam_policy_document.lambda_assume.json } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;lambda_policy\u0026#34; { statement { actions = [\u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;] resources = [\u0026#34;*\u0026#34;] } statement { actions = [\u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;] resources = [ var.products_table_arn, var.orders_table_arn, var.users_table_arn, \u0026#34;${var.products_table_arn}/index/*\u0026#34;, \u0026#34;${var.orders_table_arn}/index/*\u0026#34;, \u0026#34;${var.users_table_arn}/index/*\u0026#34; ] } statement { actions = [\u0026#34;secretsmanager:GetSecretValue\u0026#34;] resources = [var.jwt_secret_arn] } } Permissions:\nCloudWatch Logs: Create log groups and streams DynamoDB: Full CRUD operations on all three tables and their indexes Secrets Manager: Read JWT secret value Monitoring and Observability The observability module creates:\nCloudWatch Log Group: /aws/lambda/${lambda_name} CloudWatch Alarm: Triggers when error count \u0026gt; 0 SNS Topic: Receives alarm notifications Get Deployment Outputs After the pipeline completes successfully, get the API Gateway URL:\nOption 1: From GitLab CI/CD Job Logs Go to CI/CD → Pipelines Click on the completed pipeline Click on terraform_deploy job Scroll to the end of logs to find Terraform outputs: Outputs: api_url = \u0026#34;https://abc123xyz.execute-api.ap-southeast-1.amazonaws.com\u0026#34; lambda_arn = \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:fastapi-lambda-fn\u0026#34; Option 2: From AWS Console Navigate to API Gateway → APIs Find your API (name: fastapi-lambda-http) Copy the Invoke URL Option 3: Using AWS CLI # Get API Gateway URL aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?Name==`fastapi-lambda-http`].ApiEndpoint\u0026#39; \\ --output text Verify Deployment Test the health endpoint:\nAPI_URL=\u0026#34;https://abc123xyz.execute-api.ap-southeast-1.amazonaws.com\u0026#34; curl $API_URL/health Expected response:\n{\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Manual Deployment (Optional) If you need to deploy manually (not recommended):\ncd Backend-FastAPI-Docker_Build-Pipeline/infra # Initialize Terraform terraform init # Plan deployment terraform plan \\ -var \u0026#34;project_name=fastapi-lambda\u0026#34; \\ -var \u0026#34;region=ap-southeast-1\u0026#34; \\ -var \u0026#34;jwt_secret_arn=arn:aws:secretsmanager:...\u0026#34; \\ -var \u0026#34;image_uri=123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda:latest\u0026#34; # Apply deployment terraform apply \\ -var \u0026#34;project_name=fastapi-lambda\u0026#34; \\ -var \u0026#34;region=ap-southeast-1\u0026#34; \\ -var \u0026#34;jwt_secret_arn=arn:aws:secretsmanager:...\u0026#34; \\ -var \u0026#34;image_uri=123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda:latest\u0026#34; The pipeline automatically handles deployment. Manual deployment is only needed for troubleshooting or initial setup before the pipeline is configured.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "DevSecOps Security Testing &amp; Configuration",
	"tags": [],
	"description": "",
	"content": "DevSecOps Security Testing \u0026amp; Configuration This section focuses on security testing and manual configuration of security scanning tools for the FastAPI Backend. You will learn how to configure and test Semgrep, Trivy, and ECR scanning manually.\nDevSecOps Testing Overview The FastAPI Backend implements defense-in-depth security through multiple testing layers:\nLayer Tool Type Purpose Code Semgrep SAST Static code analysis Dependencies Trivy FS Dependency Scan Scan filesystem for vulnerabilities Container Trivy Image Container Scan Scan Docker image Registry ECR Scan Image Scan Automatic scan on push Step 1: Manual Semgrep Configuration \u0026amp; Testing Install Semgrep # Install Semgrep pip install semgrep # Or using Docker docker pull returntocorp/semgrep Configure Custom Rules Edit Backend-FastAPI-Docker_Build-Pipeline/backend/semgrep.yml:\nrules: - id: jwt-hardcoded-secret message: Avoid hardcoding JWT secrets; use Secrets Manager/env. severity: ERROR languages: [python] pattern: | jwt.encode($P, $S, ...) metavariable-pattern: metavariable: $S pattern: \u0026#34;\u0026#39;$SECRET\u0026#39;\u0026#34; - id: fastapi-debug message: Do not run uvicorn with reload or debug in production. severity: WARNING languages: [python] pattern-either: - pattern: uvicorn.run(..., reload=True, ...) - pattern: uvicorn.run(..., debug=True, ...) Run Semgrep Scan Manually cd Backend-FastAPI-Docker_Build-Pipeline/backend # Basic scan semgrep --config semgrep.yml . # Scan with JSON output semgrep --config semgrep.yml . --json -o semgrep-results.json # Scan with HTML report semgrep --config semgrep.yml . --html -o semgrep-report.html Step 2: Manual Trivy Configuration \u0026amp; Testing Install Trivy # Install Trivy curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin # Verify installation trivy --version Run Filesystem Scan cd Backend-FastAPI-Docker_Build-Pipeline/backend # Basic filesystem scan trivy fs --severity HIGH,CRITICAL . # Scan with JSON output trivy fs --severity HIGH,CRITICAL . --format json -o trivy-fs-results.json Build and Scan Container Image # Build Docker image cd Backend-FastAPI-Docker_Build-Pipeline docker build -t fastapi-lambda:test backend/ # Scan container image trivy image --severity HIGH,CRITICAL fastapi-lambda:test # Scan with exit code (fails on findings) trivy image --exit-code 1 --severity HIGH,CRITICAL fastapi-lambda:test Example Output:\nScanning filesystem with Trivy... 2025-11-18T03:44:45Z\tINFO\tDetected OS\tfamily=\u0026#34;amazon\u0026#34; version=\u0026#34;2\u0026#34; 2025-11-18T03:45:14Z\tINFO\t[python-pkg] Detecting vulnerabilities... Report Summary ┌──────────────────────────────────────────────────────────┬────────────┬─────────────────┬─────────┐ │ Target │ Type │ Vulnerabilities │ Secrets │ ├──────────────────────────────────────────────────────────┼────────────┼─────────────────┼─────────┤ │ fastapi-lambda:test (amazon 2) │ amazon │ 0 │ - │ │ var/task/fastapi-0.115.0.dist-info/METADATA │ python-pkg │ 1 │ - │ └──────────────────────────────────────────────────────────┴────────────┴─────────────────┴─────────┘ CRITICAL: 1 (CVE-2024-XXXXX in fastapi==0.115.0) Step 3: Manual ECR Image Scanning Configuration Enable ECR Image Scanning # Enable scanning on existing repository aws ecr put-image-scanning-configuration \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 View ECR Scan Results # Get scan findings aws ecr describe-image-scan-findings \\ --repository-name fastapi-lambda \\ --image-id imageTag=test \\ --region ap-southeast-1 Security Testing Workflow Create security-test.sh script to run all security tests:\n#!/bin/bash set -e echo \u0026#34;Starting DevSecOps Security Testing...\u0026#34; # 1. Semgrep SAST Scan echo \u0026#34;Running Semgrep SAST scan...\u0026#34; cd Backend-FastAPI-Docker_Build-Pipeline/backend semgrep --config semgrep.yml . --json -o ../semgrep-results.json # 2. Trivy Filesystem Scan echo \u0026#34;Running Trivy filesystem scan...\u0026#34; trivy fs --severity HIGH,CRITICAL . --format json -o ../trivy-fs-results.json # 3. Build and Scan Container echo \u0026#34;Building Docker image...\u0026#34; docker build -t fastapi-lambda:security-test backend/ echo \u0026#34;Running Trivy container scan...\u0026#34; trivy image --severity HIGH,CRITICAL --format json -o ../trivy-image-results.json fastapi-lambda:security-test echo \u0026#34;Security testing complete!\u0026#34; View Results in GitLab CI/CD After pushing code, view security scan results:\nNavigate to CI/CD → Pipelines Click on pipeline run View lint_and_scan job logs for Semgrep results View build_and_push job logs for Trivy results Best Practices Run Before Push: Test locally before pushing Fail Fast: Use --exit-code 1 to fail builds on critical findings Severity Threshold: Only block on HIGH and CRITICAL findings Regular Updates: Update Semgrep rules and Trivy database regularly "
},
{
	"uri": "http://localhost:1313/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before starting this workshop, ensure you have the following requirements ready.\n1. AWS Account You need an AWS account with appropriate permissions. If you don\u0026rsquo;t have one:\nGo to AWS Console Click Create an AWS Account Follow the registration process Enable MFA for the root account (recommended) This workshop will create resources that incur costs. Estimated cost is ~$5-10 if cleaned up within a few hours. Make sure to complete the cleanup section at the end.\n2. IAM User with Required Permissions Create an IAM user with programmatic access for GitLab CI/CD:\nCreate IAM User # Create IAM user aws iam create-user --user-name gitlab-ci-user # Create access key aws iam create-access-key --user-name gitlab-ci-user Save the Access Key ID and Secret Access Key - you\u0026rsquo;ll need them for GitLab CI/CD variables.\nAttach Required Policies The IAM user needs permissions for:\nLambda, API Gateway, DynamoDB ECR (Elastic Container Registry) Secrets Manager CloudWatch, SNS IAM (for creating roles) S3 (for Terraform state, if using remote backend) Option 1: Administrator Access (for workshop)\naws iam attach-user-policy \\ --user-name gitlab-ci-user \\ --policy-arn arn:aws:iam::aws:policy/AdministratorAccess Option 2: Custom Policy (recommended for production)\nCreate a custom policy with least privilege permissions for the services listed above.\n3. GitLab Account Sign up at GitLab Create a new project/repository Fork or clone the workshop repository 4. AWS CLI Configuration Install and configure AWS CLI on your local machine:\nInstall AWS CLI # Windows (PowerShell) msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi # macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configure AWS CLI aws configure Enter the following when prompted:\nAWS Access Key ID: Your IAM user access key AWS Secret Access Key: Your IAM user secret key Default region name: ap-southeast-1 (or your preferred region) Default output format: json Verify AWS CLI Configuration # Check AWS CLI version aws --version # Verify credentials aws sts get-caller-identity You should see your AWS account ID and user ARN.\n5. Clone Workshop Repository Clone the project repository:\ngit clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\n6. GitLab CI/CD Variables Setup Before running the pipeline, you need to configure GitLab CI/CD variables. This will be covered in detail in the next section, but here\u0026rsquo;s a quick overview:\nRequired Variables:\nAWS_ACCESS_KEY_ID - AWS access key AWS_SECRET_ACCESS_KEY - AWS secret key (masked) AWS_DEFAULT_REGION - AWS region JWT_SECRET_ARN - Secrets Manager ARN for JWT ECR_URI - ECR repository URI PROJECT_NAME - Project name LAMBDA_FUNCTION_NAME - Lambda function name 7. Verify Setup Run the following commands to verify your setup:\n# Check AWS CLI aws --version aws sts get-caller-identity # Check Git git --version # Check Docker (optional, for local testing) docker --version Workshop Resources The repository contains all the code and configurations needed for this workshop:\nThe workshop repository contains:\nBackend: FastAPI application code with layered architecture Infrastructure: Terraform modules for all AWS resources (DynamoDB, Lambda, API Gateway, IAM, Observability) Pipeline: GitLab CI/CD configuration (.gitlab-ci.yml) Security: Semgrep and Trivy scanning configurations Docker: Dockerfile for Lambda container image Next Steps After completing prerequisites:\n✅ AWS account created ✅ IAM user with access keys ✅ AWS CLI configured ✅ Repository cloned ⏭️ Configure GitLab CI/CD variables (next section) ⏭️ Set up GitLab CI/CD pipeline (next section) "
},
{
	"uri": "http://localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Secure Serverless Application Proposal 1. Project Summary The project operates a serverless application on AWS with a fully automated CI/CD pipeline. When code is pushed to GitLab, the pipeline automatically scans for security vulnerabilities (Semgrep, Trivy), builds containers, provisions infrastructure via Terraform, and deploys images to Amazon ECR. Users access the application through Route 53 → CloudFront → WAF → API Gateway, authenticated by Cognito, invoking Lambda functions for business logic and storing data in DynamoDB. The system continuously monitors through CloudWatch, CloudTrail, and GuardDuty, sending alerts via EventBridge → SNS upon incidents or threats.\n2. Problem Statement Current Problems Many organizations struggle with deploying serverless applications due to error-prone manual processes, lack of security controls, and scalability challenges. Specific issues include:\nManual deployment processes lack consistent security checks, often missing vulnerability scanning and automated testing steps Existing systems do not provide high-performance content delivery (CDN) nor application-layer protection against DDoS and bot attacks Secrets, encryption keys, and IAM permissions are difficult to manage end-to-end across the CI/CD pipeline while maintaining least-privilege principles Limited observability causes slow incident response and delayed threat detection Solution The project delivers a complete serverless architecture with automated CI/CD, integrated security, and comprehensive monitoring. The deployment pipeline automates from GitLab through CodePipeline, CodeBuild, Semgrep (SAST), Trivy (container scanning), Terraform (IaC), and CodeDeploy, ensuring every change is tested and security-scanned. The delivery layer uses Route 53, CloudFront, and WAF to accelerate and protect the application. The serverless backend with Cognito (authentication), API Gateway, Lambda, DynamoDB, Secrets Manager, and KMS ensures data security. A multi-layer monitoring system (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) detects and alerts on incidents instantly.\nBenefits and Return on Investment (ROI) The solution delivers significant technical and financial benefits: full automation reduces deployment time by 80-90% and eliminates manual errors, integrated security reduces vulnerability risks, CDN and serverless architecture ensure high performance with automatic scaling, and centralized monitoring enables 70-80% faster incident detection. Financially, operational costs are estimated at ~$20-40/month for small to medium scale, significantly lower than traditional infrastructure, with no upfront hardware investment and 60-70% reduction in operational labor costs. Expected payback period is 1-2 months, while providing an extensible foundation for future projects.\n3. System Architecture The architecture is divided into three domains:\nCI/CD Pipeline: Running on GitLab and AWS CodePipeline to build containers, scan security (Semgrep, Trivy), provision infrastructure via Terraform, and deploy images to Amazon ECR Content Delivery \u0026amp; Protection Layer: Using Route 53, AWS WAF, and CloudFront to accelerate and secure user access Serverless Application Core: Located in ap-southeast-1 region with Cognito (authentication), API Gateway, Lambda (business logic), DynamoDB (storage), KMS and Secrets Manager (security), and the observability suite (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) AWS Services Used\nGitLab Actions, AWS CodePipeline, CodeBuild, CodeDeploy Semgrep, Trivy Terraform, Amazon ECR Amazon Cognito, Amazon API Gateway, AWS Lambda, Amazon DynamoDB AWS WAF, Amazon CloudFront, Amazon Route 53 AWS Secrets Manager, AWS Key Management Service (KMS) Amazon CloudWatch, AWS CloudTrail, Amazon GuardDuty, Amazon EventBridge, Amazon SNS Component Design\nCI/CD: A Git push triggers the pipeline to run Semgrep (SAST), build containers in CodeBuild, scan with Trivy, execute Terraform Plan/Apply, and deliver via CodeDeploy. Delivery \u0026amp; Protection: Route 53 handles DNS → CloudFront caches content → WAF filters malicious traffic before reaching the APIs. Application Services: Cognito issues tokens, API Gateway validates requests and forwards them to Lambda, which processes business logic and reads/writes DynamoDB. Secrets \u0026amp; Encryption: Secrets Manager stores sensitive information, KMS encrypts data and keys. Monitoring \u0026amp; Alerting: CloudWatch aggregates metrics/logs, CloudTrail records audit trails, GuardDuty detects threats, EventBridge routes anomalies to SNS notifications. 4. Technical Implementation Model the infrastructure with Terraform: logical VPC boundaries, least-privilege IAM roles, API Gateway, Lambda, DynamoDB, and security controls. Configure GitLab CI/CD with AWS CodePipeline/CodeBuild/CodeDeploy using cross-account IAM roles. Enforce Semgrep and Trivy scans in every pipeline run, failing builds on high-severity findings. Define Terraform modules for Lambda (container images in ECR), API Gateway, CloudFront, WAF, and Cognito resources. Enable CloudWatch log groups, metric filters, GuardDuty, CloudTrail; create EventBridge rules that fan out to SNS alerts. Implement automated rollback via CodeDeploy deployment groups and versioned Terraform state. 5. Timeline \u0026amp; Milestones Phase Schedule Key Deliverables Kick-off Week 1 Requirements intake, detailed design, IAM role matrix IaC \u0026amp; CI/CD Setup Week 2 Terraform baseline, GitLab pipeline, CodePipeline integration Security Integration Week 3 Semgrep, Trivy, WAF rules, Cognito, Secrets Manager Backend Completion Week 4 Lambda handlers, API Gateway routes, DynamoDB schema, unit tests Deployment \u0026amp; Testing Week 5 End-to-end pipeline run, integration and CDN performance testing Operations \u0026amp; Handover Week 6 Optimize monitoring systems, configure intelligent alerting, finalize documentation Post-Deployment Week 7+ Continuous performance and cost monitoring, configuration optimization, feature expansion as needed, periodic security updates 6. Budget Estimation AWS Service Primary Billing Factor Estimated Cost (USD) AWS Lambda \u0026amp; API Gateway 250,000 requests/month ~$1.00 - $3.00 Amazon DynamoDB 5 GB storage, 5 RCU/5 WCU (Provisioned) ~$5.00 - $10.00 CI/CD (CodePipeline/CodeBuild) 5 deployments (250 build minutes) ~$2.25 - $4.00 Amazon ECR 5 GB image storage ($0.10/GB) ~$0.50 - $1.50 Route 53 1 Hosted Zone + queries ~$0.54 CloudFront / WAF 15 GB Data Transfer Out, basic WAF usage ~$9.50 - $15.50 Security \u0026amp; Monitoring GuardDuty, KMS, Secrets Manager, minimal logs ~$2.00 - $5.00 Total Cost ~$20.79 - $39.54 USD/month Note: Actual spend varies with traffic and configuration; costs can drop further by shutting down dev/test environments. No hardware expenditure required.\n7. Risk Assessment Risk Impact Likelihood Mitigation Undetected vulnerabilities High Medium Mandatory Semgrep/Trivy scans, enable AWS Inspector, periodic manual reviews Misconfigured Terraform causing downtime High Low Peer review Terraform plans, use staging environments, back up state Unexpected CDN or Terraform cost spikes Medium Medium Configure AWS Budgets, tune CloudFront cache policies, monitor spend Cognito authentication issues Medium Low Implement automated auth tests, maintain backup user pool in another Region Mitigation Strategies Security: Integrate automated security scanning (Semgrep, Trivy) into every pipeline, enable AWS Inspector periodically, conduct code reviews and quarterly security audits Infrastructure Configuration: Apply Infrastructure as Code (Terraform) with version control, peer review for all changes, use staging environments for testing before production, regularly backup Terraform state Cost Management: Set up AWS Budgets with 80% and 100% alert thresholds, configure optimized CloudFront cache policies, monitor costs daily via Cost Explorer, automatically shut down dev/test resources when unused Authentication \u0026amp; Access: Deploy automated testing for API authentication, maintain backup Cognito user pool in another region, enable multi-factor authentication (MFA) for administrative accounts Contingency Plan Security Incidents: Upon detection of critical vulnerabilities, immediately rollback to previous version via CodeDeploy, isolate affected resources, notify security team and proceed with patching Downtime from Misconfiguration: Use Terraform state backup to restore previous configuration, temporarily route traffic to staging environment if needed, maintain runbook for quick rollback Budget Overrun: Automatically shut down non-essential services when alert threshold is reached, optimize CloudFront cache hit ratio, consider switching to Reserved Capacity for DynamoDB if long-term usage Authentication Failures: Switch to backup Cognito user pool in another region, utilize API Gateway caching to reduce load, maintain fallback authentication mechanism 8. Expected Outcomes A secure, fully automated CI/CD pipeline where every change is scanned and tested before reaching production. A low-latency serverless backend served through CDN and API Gateway with pervasive data encryption. Centralized observability that surfaces incidents instantly and supports high availability SLAs. Optimized access control and cost efficiency delivered through least-privilege IAM and serverless scaling. An extensible infrastructure foundation ready for future features or additional AWS Regions. "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Report: AI/ML/GenAI on AWS Event Objectives Introduce an overview of AI/ML and GenAI on AWS.\nHelp participants understand and get hands-on experience with SageMaker and Bedrock.\nGuide building practical GenAI solutions such as RAG and chatbots.\nSpeakers Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, FCJ Lam Tuan Kiet - Sr DevOps Engineer, Fpt Software Truong Quang Tinh - Platform Engineer, TymeX Key Highlights Prompting techniques for LLMs Zero shot prompting: the model infers without examples Few shot prompting: using a few examples to improve accuracy and consistency Chain of thought prompting: guides the model to think step-by-step to improve reasoning Retrieval Augmented Generation (RAG) Overview of RAG mechanism RAG use cases: enterprise chatbots, document search, report analysis assistants, technical support How to build a RAG pipeline Embedding and Amazon Titan Embeddings Embedding: vector representation of data to help the model understand semantics Applications of embeddings in search, clustering, semantic similarity, and RAG Amazon Titan Embedding: high accuracy, optimized for RAG, supports multi-language Amazon Bedrock Bedrock is AWS’s fully managed GenAI platform Supports multiple foundation models and Titan models Provides a unified API and enterprise-level data security Built-in features for Guardrails, RAG, Memory, and Agents Agentic AI AI is evolving from passive responses to agentic AI, capable of: Planning Goal-oriented actions Tool calling Multi-system communication Bedrock Agent Core Agent-centric architecture to run multiple agents at scale Supports: Routing between agents Tool calling orchestration Retrieval integration Runtime guardrails Agent Core service enabling agents at scale Allows deployment of agents with: High availability Scalable worker orchestration Built-in monitoring and governance Easy integration with enterprise systems Key Takeaways Prompting Zero shot is suitable for simple tasks Few shot improves accuracy in complex tasks Chain of thought helps the model explain and reason better for logical problems Bedrock and Agentic AI Bedrock enables enterprises to deploy GenAI without managing infrastructure Agentic AI is the future for enterprise automation Bedrock Agent Core simplifies creating, managing, and scaling AI agents Applying to Work Use Zero shot, Few shot, and Chain of thought to improve prompting quality Event Experience Attending the event provided a fresh perspective on building modern AI systems, from prompting techniques to RAG and agents.\nLearning from experts Experts explained the architecture of prompting, embedding, and RAG clearly Real-world case studies demonstrated enterprises using GenAI for operations and automation Hands-on demonstrations Live demo of building a Generative AI chatbot using Bedrock Lessons learned Good prompting determines output quality Agentic AI is essential for advanced automation Bedrock Agent Core enables building strong, secure, and scalable agents Event moments The event broadened my perspective on GenAI, especially the usefulness of intelligent AI agents and applying them to enterprise workflows.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Practice AWS core services including tags, resource groups, cloudwatch, and auto scaling. Deploy wordpress on lightsail and manage resources with AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about tags and resource groups - Practice: + Add tags to resources + Create and manage a resource group 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Explore Amazon cloudwatch - Understand cloudwatch metrics - Practice: + View and analyze EC2 logs in cloudwatch + Create cloudwatch alarms + \u0026hellip; 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about auto scaling groups - Practice: + Set up a lab environment + Configure a load balancer + Create and test an auto scaling group 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon lightsail - Get familiar with wordpress - Practice: + Launch a wordpress instance + deploy and manage wordpress + Create an instance snapshot in Amazon lightsail + \u0026hellip; 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about the AWS CLI - Explore AWS cloud9 - Practice: + Install and configure the CLI + Create and manage resources using CLI commands 09/26/2025 09/27/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood the concept of tags and resource groups in AWS:\nLearned how tags support cost allocation and environment grouping Gained the ability to create and manage resource groups for centralized management Understood the role of cloudwatch in monitoring AWS resources and applications.\nBecame familiar with auto scaling group and how to configure and test auto scaling policies to automatically adjust capacity.\nUnderstood how lightsail simplifies application deployment:\nLearned how to deploy and manage WordPress instances Practiced creating snapshots for backup and recovery \u0026hellip; Learned to use CLI commands to manage resources as an alternative to the web console.\nBecame familiar with cloud9 as a cloud-based IDE.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/",
	"title": "Setting up GitLab CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Setting up GitLab CI/CD Pipeline In this section, you will set up a GitLab CI/CD pipeline to automate building, security scanning, and deploying your FastAPI application.\nPipeline Overview The GitLab CI/CD pipeline consists of three stages:\nStage Job Tool Purpose lint lint_and_scan Semgrep Static Application Security Testing (SAST) build build_and_push Docker + Trivy Build container image and scan for vulnerabilities deploy terraform_deploy Terraform Provision/update infrastructure Content Setup GitLab CI/CD Pipeline - Config AWS CLI, GitLab variables, and pipeline Security Scanning Configuration - Semgrep and Trivy integration Pipeline Flow After the pipeline completes successfully, you should see all stages with green checkmarks:\nGitLab CI/CD Configuration The pipeline is defined in .gitlab-ci.yml:\nstages: - lint - build - deploy lint_and_scan: stage: lint script: - semgrep --config backend/semgrep.yml backend/ build_and_push: stage: build script: - docker build -t fastapi-lambda:$CI_COMMIT_SHORT_SHA backend - trivy fs --severity HIGH,CRITICAL . - trivy image --severity HIGH,CRITICAL fastapi-lambda:$CI_COMMIT_SHORT_SHA - docker push $ECR_URI:$CI_COMMIT_SHORT_SHA terraform_deploy: stage: deploy script: - cd infra - terraform init - terraform apply -auto-approve Security Scanning The pipeline includes two types of security scanning:\nSemgrep (SAST): Static code analysis for security vulnerabilities Trivy: Container vulnerability scanning (filesystem + image) Key Features ✅ Automatic Trigger: Runs on every push to main branch ✅ Security First: Fails build on critical security findings ✅ Container Image: Builds and scans Docker image ✅ Infrastructure as Code: Terraform automatically deploys infrastructure ✅ GitLab Variables: All secrets stored in GitLab CI/CD variables Quick Setup Config AWS CLI (local) Create JWT Secret (AWS Secrets Manager) Create ECR Repository (AWS ECR) Config GitLab Variables (7 variables) Push Code → Pipeline runs automatically The pipeline automatically handles everything: build, scan, and deploy. You only need to configure AWS CLI and GitLab variables once.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Testing the API",
	"tags": [],
	"description": "",
	"content": "Testing the API After the pipeline deploys the backend, you can test all API endpoints.\nStep 1: Get API Gateway URL # Option 1: From GitLab job logs (terraform_deploy job) # Look for: api_url = \u0026#34;https://...\u0026#34; # Option 2: From Terraform output cd Backend-FastAPI-Docker_Build-Pipeline/infra API_URL=$(terraform output -raw api_url) echo $API_URL Example: https://0bb7ewqnga.execute-api.ap-southeast-1.amazonaws.com\nStep 2: Test Health Endpoint curl $API_URL/health Expected response:\n{\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Step 3: Access Swagger UI Open in browser:\nhttps://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs The Swagger UI allows you to:\nView all available endpoints Test endpoints directly from the browser See request/response schemas Try out authentication Step 4: Register a User curl -X POST $API_URL/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34; }\u0026#39; Expected response:\n{\u0026#34;message\u0026#34;: \u0026#34;registered\u0026#34;} Step 5: Login and Get JWT Token curl -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Save the token:\nTOKEN=$(curl -s -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34;}\u0026#39; \\ | jq -r \u0026#39;.access_token\u0026#39;) echo \u0026#34;Token: $TOKEN\u0026#34; Step 6: List Products (Public) curl $API_URL/products Expected response:\n[] Step 7: Create Product (Admin Only) Note: Regular users cannot create products. You need an admin user. For testing, you can manually create an admin user in DynamoDB or modify the registration logic.\ncurl -X POST $API_URL/products \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 999.99, \u0026#34;stock\u0026#34;: 10, \u0026#34;category\u0026#34;: \u0026#34;electronics\u0026#34; }\u0026#39; If user is not admin, you\u0026rsquo;ll get:\n{\u0026#34;detail\u0026#34;: \u0026#34;Admin only\u0026#34;} Step 8: Get Product by ID PRODUCT_ID=\u0026#34;your-product-id\u0026#34; curl $API_URL/products/$PRODUCT_ID Step 9: List Products by Category curl \u0026#34;$API_URL/products?category=electronics\u0026#34; Step 10: Create Order curl -X POST $API_URL/orders \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;items\u0026#34;: [ { \u0026#34;product_id\u0026#34;: \u0026#34;product-id-here\u0026#34;, \u0026#34;quantity\u0026#34;: 2 } ] }\u0026#39; Expected response:\n{ \u0026#34;order_id\u0026#34;: \u0026#34;uuid-here\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-id-here\u0026#34;, \u0026#34;items\u0026#34;: [...], \u0026#34;total_amount\u0026#34;: 1999.98, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-01T00:00:00Z\u0026#34; } Step 11: List User Orders curl -X GET $API_URL/orders \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Step 12: Get Order by ID ORDER_ID=\u0026#34;your-order-id\u0026#34; curl -X GET $API_URL/orders/$ORDER_ID \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Testing in Browser You can test public endpoints directly in browser:\nHealth: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/health Products: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/products Swagger: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs View Resources in AWS Console Lambda Function Navigate to Lambda Console Find: fastapi-lambda-fn View configuration, logs, and metrics API Gateway Navigate to API Gateway Console Find your API (name: fastapi-lambda-http) View routes, stages, and logs DynamoDB Tables Navigate to DynamoDB Console View tables: products, orders, users Explore table items and indexes CloudWatch Logs Navigate to CloudWatch Logs Find: /aws/lambda/fastapi-lambda-fn View logs with correlation IDs Common Test Scenarios Authentication Flow # Register → Login → Use Token curl -X POST $API_URL/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;test@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;pass123\u0026#34;}\u0026#39; TOKEN=$(curl -s -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;test@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;pass123\u0026#34;}\u0026#39; \\ | jq -r \u0026#39;.access_token\u0026#39;) curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $API_URL/products Error Handling # Invalid credentials curl -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;wrong@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;wrong\u0026#34;}\u0026#39; # Missing token curl $API_URL/orders # Invalid token curl -H \u0026#34;Authorization: Bearer invalid-token\u0026#34; $API_URL/orders Use correlation IDs from response headers (X-Correlation-ID) to trace requests across CloudWatch logs.\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: DevOps on AWS Event Objectives Present the CI/CD ecosystem on AWS using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Introduce Infrastructure as Code (IaC) with CloudFormation and AWS CDK Explain container deployment using Docker, ECR, ECS, EKS, and App Runner Demonstrate how to set up Monitoring and Observability with CloudWatch and AWS X-Ray Share DevOps best practices, case studies, and career pathways Speakers Bao Huynh - AWS Community Builders Thinh Nguyen - AWS Community Builders Vi Tran - AWS Community Builders Key Highlights AWS CI/CD Pipeline Source Control: CodeCommit Build \u0026amp; Test: CodeBuild pipelines with unit and integration tests Deployment: CodeDeploy supporting Blue/Green, Canary, and Rolling updates Automation: CodePipeline orchestrating the full CI/CD workflow Infrastructure as Code (IaC) AWS CloudFormation: templates, stack lifecycle, drift detection AWS CDK: constructs, reusable patterns, multi-language support Demo: deploying resources using both CloudFormation and CDK Discussion: choosing the right IaC tool based on project requirements Container Services on AWS Docker fundamentals: containerization and microservices Amazon ECR: image scanning and lifecycle policies Amazon ECS \u0026amp; EKS: orchestration, scaling, service discovery AWS App Runner: simplified container deployment without infrastructure management Monitoring and Observability CloudWatch: metrics, logs, alarms, dashboards AWS X-Ray: distributed tracing and end-to-end performance analysis Key Learnings DevOps accelerates deployment cycles Automated CI/CD pipelines reduce errors and speed up releases Blue/Green and Canary deployments lower deployment risks IaC is the foundation of modern DevOps Infrastructure must be defined as code for consistency, auditing, and automation CDK accelerates development through reusable and maintainable constructs Containerization enables scalable microservices ECS/EKS are well-suited for large and complex systems App Runner is ideal for teams prioritizing simplicity and fast deployment Observability is essential CloudWatch + X-Ray provide complete system visibility Proper alerting significantly reduces MTTR Applying to Work Implement CodePipeline for automated build–test–deploy workflows Use Blue/Green deployments to minimize downtime Adopt CloudFormation for consistent and auditable infrastructure Create CloudWatch dashboards to monitor service performance Deploy microservices with ECS integrated with CodePipeline for full automation Event Experience Attending the “DevOps on AWS” event offered valuable real-world insights into applying DevOps in a cloud-native environment.\nDeep-dive sessions Hands-on demos for CI/CD and IaC Clear understanding of the differences among ECS, EKS, and App Runner Lessons Learned DevOps = culture + processes + tools IaC and CI/CD are the core pillars Observability must be implemented from the beginning Containers are the future of application deployment Event moments Overall, the event was highly informative, practical, and provided clear direction for advancing a professional DevOps career.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Strengthen understanding of core AWS services including DynamoDB, CloudFront, and Amazon WorkSpaces through hands-on practice. Learn to deploy, manage, and integrate cloud-based solutions for better performance. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about DynamoDB - Advantages of dynamodb over other databases - Practice: + Create and manage tables in AWS management console + Used AWS CloudShell to perform CRUD operations 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon WorkSpaces - Study the concept and use cases of virtual desktops on AWS - Practice: + Set up the lab environment and deploy a WorkSpace + Remotely access and test the WorkSpace environment 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Set up CloudFront with S3 as the origin - Deployed a CloudFront distribution connected to an S3 bucket 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS CloudFront: + Learn how CloudFront improves content delivery and reduces latency + Creat CloudFront distributions and configured origin groups + Deploy a Lambda@Edge function for dynamic content customization + \u0026hellip; 10/02/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Amazon Nova Canvas (Virtual Try-On) - Write sample code to generate AI-powered virtual try-on images 10/04/2025 10/04/2025 https://aws.amazon.com/vi/blogs/aws/amazon-nova-canvas-update-virtual-try-on-and-style-options-now-available/ Week 4 Achievements: Enhanced Knowledge of NoSQL Databases:\nGained a solid understanding of DynamoDB’s architecture, including tables and partition keys Completed practical exercises to manage data efficiently using both Console and CloudShell Successfully Deployed Amazon WorkSpaces\nConfigured IAM permissions and network settings Deployed a cloud-based virtual desktop and connected to it remotely Implemented CloudFront for global content delivery:\nCreated and configured CloudFront distributions for high-speed, secure content delivery Built an origin group Deployed Lambda@Edge functions \u0026hellip; Integrated CloudFront with S3: configured S3 as an origin source for static content distribution\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: Vietnam Cloud Day 2025\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/",
	"title": "FastAPI Backend Deployment",
	"tags": [],
	"description": "",
	"content": "FastAPI Backend Deployment The FastAPI backend is automatically deployed by the GitLab CI/CD pipeline. This section explains the application structure and how to test it.\nArchitecture Overview The application follows a clean layered architecture:\nLayer Location Responsibility API app/api/routers/ HTTP request handling, routing Service app/services/ Business logic, orchestration Repository app/repositories/ Data access, DynamoDB operations Models app/models/ Data structures, validation Core app/core/ Configuration, security, logging Request Flow HTTP Request ↓ API Gateway HTTP API ↓ Lambda Function (Container) ↓ Mangum (ASGI Adapter) ↓ FastAPI Application ├── /auth (register, login) ├── /products (CRUD operations) └── /orders (create, list, get) ↓ Service Layer (Business Logic) ↓ Repository Layer (DynamoDB) ↓ DynamoDB Tables Content FastAPI Application Structure - Code structure and architecture Infrastructure Deployment - Terraform modules and AWS resources Testing the API - Test endpoints with curl and Swagger UI Load Testing - Performance testing with Artillery Automatic Deployment The terraform_deploy job in GitLab CI/CD automatically:\nInitializes Terraform Plans infrastructure changes Applies changes (creates/updates resources) Outputs API Gateway URL No manual deployment needed! Just push code and the pipeline handles everything.\nInfrastructure Created DynamoDB Tables: products, orders, users Lambda Function: Container-based FastAPI application API Gateway: HTTP API (v2) with Lambda integration IAM Roles: Least privilege permissions CloudWatch: Log groups and alarms SNS: Alert notifications Get API URL After pipeline completes:\n# From GitLab job logs (terraform_deploy) # Or from Terraform output: cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url Test Health Endpoint API_URL=\u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; curl $API_URL/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} API Endpoints Method Endpoint Description Auth GET /health Health check Public POST /auth/register Register user Public POST /auth/login Login and get JWT Public GET /products List products Public POST /products Create product Admin GET /products/{id} Get product Public PUT /products/{id} Update product Admin DELETE /products/{id} Delete product Admin GET /orders List user orders User POST /orders Create order User GET /orders/{id} Get order User Swagger Documentation Access interactive API documentation:\nhttps://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs The backend is automatically deployed by the pipeline. You only need to test it after deployment completes.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Load Testing with Artillery",
	"tags": [],
	"description": "",
	"content": "Load Testing with Artillery In this section, you will perform load testing on the deployed API using Artillery to simulate user traffic and measure performance.\nPrerequisites Install Artillery:\nnpm install -g artillery Or using Docker:\ndocker run -it --rm -v $(pwd):/work artilleryio/artillery:latest Step 1: Get API URL and Token # Get API URL cd Backend-FastAPI-Docker_Build-Pipeline/infra API_URL=$(terraform output -raw api_url) # Get admin token (you need to login first) TOKEN=\u0026#34;YOUR_ADMIN_TOKEN_HERE\u0026#34; Note: Replace with your actual admin token from /auth/login endpoint.\nStep 2: Create Load Test Configuration Create load-test.yml:\nconfig: target: \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; phases: - duration: 60 arrivalRate: 10 name: \u0026#34;Warm up\u0026#34; - duration: 120 arrivalRate: 50 name: \u0026#34;Ramp up\u0026#34; - duration: 60 arrivalRate: 10 name: \u0026#34;Cool down\u0026#34; defaults: headers: Authorization: \u0026#34;Bearer YOUR_ADMIN_TOKEN_HERE\u0026#34; Content-Type: \u0026#34;application/json\u0026#34; scenarios: - name: \u0026#34;Get products\u0026#34; weight: 70 flow: - get: url: \u0026#34;/products\u0026#34; - name: \u0026#34;Create product\u0026#34; weight: 30 flow: - post: url: \u0026#34;/products\u0026#34; json: name: \u0026#34;Load test product {{ $randomString() }}\u0026#34; price: 29.99 stock: 100 category: \u0026#34;electronics\u0026#34; Important: Replace:\ntarget with your actual API Gateway URL Authorization header with your admin token Step 3: Run Load Test artillery run load-test.yml Step 4: Analyze Results Artillery will output statistics like:\nSummary report @ 14:30:00(+0000) 2024-01-01 Scenarios launched: 1000 Scenarios completed: 1000 Requests completed: 2000 Mean response/sec: 45.2 Response time (msec): min: 45 max: 1200 median: 120 p95: 350 p99: 800 Scenario counts: Get products: 700 (70%) Create product: 300 (30%) Codes: 200: 1950 201: 50 500: 0 Step 5: Generate HTML Report artillery run --output report.json load-test.yml artillery report report.json This generates an HTML report with:\nResponse time graphs Request rate charts Error rates Scenario breakdown Advanced Load Test Configuration For more realistic testing:\nconfig: target: \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; phases: - duration: 120 arrivalRate: 5 name: \u0026#34;Warm up\u0026#34; - duration: 300 arrivalRate: 20 name: \u0026#34;Sustained load\u0026#34; - duration: 60 arrivalRate: 5 name: \u0026#34;Cool down\u0026#34; defaults: headers: Authorization: \u0026#34;Bearer YOUR_TOKEN\u0026#34; Content-Type: \u0026#34;application/json\u0026#34; processor: \u0026#34;./processor.js\u0026#34; # Custom processor for dynamic data scenarios: - name: \u0026#34;Product browsing\u0026#34; weight: 60 flow: - get: url: \u0026#34;/products\u0026#34; - think: 2 # Wait 2 seconds - get: url: \u0026#34;/products?category=electronics\u0026#34; - name: \u0026#34;Product management\u0026#34; weight: 30 flow: - post: url: \u0026#34;/products\u0026#34; json: name: \u0026#34;Product {{ $randomString() }}\u0026#34; price: {{ $randomNumber(10, 1000) }} stock: {{ $randomNumber(1, 100) }} category: \u0026#34;{{ $pick([\u0026#39;electronics\u0026#39;, \u0026#39;clothing\u0026#39;, \u0026#39;books\u0026#39;]) }}\u0026#34; - think: 1 - get: url: \u0026#34;/products\u0026#34; - name: \u0026#34;Order flow\u0026#34; weight: 10 flow: - get: url: \u0026#34;/products\u0026#34; - post: url: \u0026#34;/orders\u0026#34; json: items: - product_id: \u0026#34;{{ $pick([\u0026#39;id1\u0026#39;, \u0026#39;id2\u0026#39;, \u0026#39;id3\u0026#39;]) }}\u0026#34; quantity: {{ $randomNumber(1, 5) }} Monitor During Load Test CloudWatch Metrics Navigate to CloudWatch → Metrics → AWS/Lambda Select fastapi-lambda-fn Monitor: Invocations: Request count Duration: Response time Errors: Error rate Throttles: Throttled requests CloudWatch Logs # Watch logs in real-time aws logs tail /aws/lambda/fastapi-lambda-fn --follow --region ap-southeast-1 Performance Benchmarks Expected performance for Lambda container:\nMetric Target Notes P50 (median) \u0026lt; 200ms Most requests P95 \u0026lt; 500ms 95% of requests P99 \u0026lt; 1000ms 99% of requests Error Rate \u0026lt; 0.1% Very low errors Throughput 50+ req/s Depends on concurrency Troubleshooting High Response Times Check Lambda cold starts Monitor DynamoDB throttling Review CloudWatch metrics Check API Gateway latency High Error Rates Check CloudWatch logs for errors Verify DynamoDB table capacity Check Lambda timeout settings Review API Gateway limits Throttling Increase Lambda concurrency limit Check DynamoDB on-demand capacity Review API Gateway throttling settings Best Practices Start Small: Begin with low arrival rates and gradually increase Monitor Resources: Watch CloudWatch metrics during testing Test Realistic Scenarios: Mix read and write operations Use Correlation IDs: Track requests across logs Test Error Handling: Include negative test cases Run load tests during off-peak hours and monitor AWS costs. Load testing can generate significant traffic and incur costs.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Well-Architected Security Pillar Event Objectives Manage secure access and permissions (IAM), including SSO, roles, and policies Implement Detection and Monitoring to detect threats early and automate alerts Protect infrastructure and workloads using network design, Security Groups, NACLs, WAF, and Shield Encrypt data and manage secrets (Data Protection) Standardize Incident Response processes and automate response actions Speakers Tran Duc Anh - Cloud Security Engineer Trainee Nguyen Tuan Thinh - Cloud Engineer Trainee Nguyen Do Thanh Dat - Cloud Engineer Trainee Thinh Lam - FCJer Viet Nguyen - FCJer Mendel Grabski - Head of Security and DevOps Key Highlights Identity and Access Management (IAM) IAM: Users, Roles, Policies – remove long-term credentials IAM Identity Center: SSO, permission sets Multi-account best practices: Service Control Policies (SCP) for organization guardrails Permission boundaries to limit privilege escalation Security enhancements: Mandatory MFA Credential rotation Detection and Continuous Monitoring AWS CloudTrail at organization level Amazon GuardDuty for advanced threat detection Logging: VPC Flow Logs ALB Logs S3 Access Logs Alerting \u0026amp; Automation: EventBridge + SNS Lambda remediation Apply Detection-as-Code using IaC Infrastructure Protection Network design: Public/Private subnets VPC segmentation Security Groups (stateful) vs NACLs (stateless) Network protection: AWS WAF AWS Shield AWS Network Firewall Data Protection AWS KMS Encryption at-rest \u0026amp; in-transit Secrets management: Secrets Manager Parameter Store Data classification and access guardrails Incident Response AWS Incident Response lifecycle Common scenarios: Compromised IAM key Public S3 exposure EC2 malware detection Response workflow: Snapshot resources Isolate workloads Collect evidence Automation: Lambda Step Functions Auto-remediation Key Learnings IAM is the foundation Remove long-term credentials Multi-account + SCP increases control Detection is essential CloudTrail + GuardDuty allow early detection Full logging supports forensic analysis Infrastructure requires defense in depth Combine SGs, NACLs, WAF, Shield, Network Firewall Data must be protected and managed Use KMS for encryption Secrets Manager replaces hard-coded secrets Incident Response needs a clear process Prepare playbooks in advance Applying to Work Enable GuardDuty and Security Hub across all accounts Configure EventBridge + Lambda for auto-remediation Enable encryption by default for S3/EBS/RDS Event Experience Attending the “AWS Well-Architected Security Pillar” event provided practical and comprehensive insights into cloud security practices on AWS.\nHighlights Deep understanding of Zero Trust \u0026amp; IAM Guidance on enterprise-grade logging \u0026amp; alerting Hands-on incident response demo Lessons Learned Cloud security = processes + people + tools IAM and Detection are top priorities Automating IR is key to reducing MTTR Defense in depth is mandatory Event moments The event was highly informative, providing actionable knowledge for cloud engineers, security specialists, and system administrators to improve AWS security operations\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand AWS organizations, IAM roles, and permission boundaries.\nGain hands-on experience with KMS, cloudtrail and analyzing logs with athena.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS organizations and multi-account strategy - Understand the purpose of core accounts - Practice: + Create AWS accounts + Set up organizational units + Apply customer managed policies + Invite AWS accounts into the organization 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about IAM permission boundary - Learn about IAM role and how to write IAM policies - Practice: + Create a restriction policy and test permission boundaries + Create IAM roles + Write and attach JSON-based policies + Test role assumptions and verify permissions 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn how to manage EC2 access using resource tags - Learn about AWS security hub - Practice: + Enable security hub + Review security standards 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS key management service - Understand symmetric vs asymmetric keys and their use cases - Learn about encrypting data in Amazon S3 using KMS keys - \u0026hellip; 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create AWS cloudtrail to record account activities + Enable logging and store logs in Amazon S3 + Create Amazon athena to analyze cloudtrail logs + Retrieve and query data from athena 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Explored AWS organizations and multi-account strategy:\nBenefits of multi-account Purposes of core accounts Organizational units Customer managed policies \u0026hellip; Became familiar with IAM roles and permission boundaries\nPracticed managing EC2 access with resource tags\nBecame familiar with writing IAM policies using JSON\nConfigured and enabled AWS security hub\nAcquired hands-on experience with AWS key management service:\nUnderstood symmetric vs asymmetric encryption keys\nLearned how KMS integrates with other AWS services\nUsed KMS keys to encrypt and protect data in Amazon S3\nImplemented cloudtrail logging and athena analysis for audit insights\nAcquired the ability to monitor and audit AWS activities effectively\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.5-policy/",
	"title": "Manual Configuration: Monitoring &amp; Security",
	"tags": [],
	"description": "",
	"content": "Manual Configuration: Monitoring \u0026amp; Security This section provides step-by-step manual configuration of CloudWatch monitoring, alarms, SNS notifications, and security settings for the FastAPI Backend.\nStep 1: Manual CloudWatch Log Group Configuration Create Log Group # Create CloudWatch log group for Lambda aws logs create-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 # Set retention period (14 days) aws logs put-retention-policy \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --retention-in-days 14 \\ --region ap-southeast-1 View Logs # List log streams aws logs describe-log-streams \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 \\ --order-by LastEventTime \\ --descending \\ --max-items 10 # Get recent log events aws logs tail /aws/lambda/fastapi-lambda-fn \\ --follow \\ --region ap-southeast-1 CloudWatch Insights Query # Query logs using CloudWatch Insights aws logs start-query \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s) \\ --end-time $(date +%s) \\ --query-string \u0026#39;fields @timestamp, @message, correlation_id | filter @message like /error/ | sort @timestamp desc | limit 20\u0026#39; \\ --region ap-southeast-1 # Get query results (replace QUERY_ID) aws logs get-query-results \\ --query-id QUERY_ID \\ --region ap-southeast-1 Step 2: Manual CloudWatch Metrics Configuration View Lambda Metrics # List available metrics aws cloudwatch list-metrics \\ --namespace AWS/Lambda \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --region ap-southeast-1 # Get metric statistics aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum \\ --region ap-southeast-1 Create Custom Metric # In your FastAPI application import boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # Publish custom metric cloudwatch.put_metric_data( Namespace=\u0026#39;FastAPI/Products\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;ProductsCreated\u0026#39;, \u0026#39;Value\u0026#39;: 1, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Timestamp\u0026#39;: datetime.utcnow() } ] ) Step 3: Manual CloudWatch Alarm Configuration Create SNS Topic First # Create SNS topic for alerts aws sns create-topic \\ --name fastapi-lambda-alerts \\ --region ap-southeast-1 # Get topic ARN SNS_TOPIC_ARN=$(aws sns list-topics \\ --region ap-southeast-1 \\ --query \u0026#39;Topics[?contains(TopicArn, `fastapi-lambda-alerts`)].TopicArn\u0026#39; \\ --output text) echo \u0026#34;SNS Topic ARN: $SNS_TOPIC_ARN\u0026#34; Create CloudWatch Alarm for Lambda Errors # Create alarm for Lambda errors aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-5xx \\ --alarm-description \u0026#34;Alert when Lambda errors exceed threshold\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 Create Alarm for High Duration # Create alarm for Lambda duration aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-duration \\ --alarm-description \u0026#34;Alert when Lambda duration is high\u0026#34; \\ --metric-name Duration \\ --namespace AWS/Lambda \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5000 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 Create Alarm for Throttles # Create alarm for Lambda throttles aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-throttles \\ --alarm-description \u0026#34;Alert when Lambda is throttled\u0026#34; \\ --metric-name Throttles \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 View Alarms # List all alarms aws cloudwatch describe-alarms \\ --alarm-name-prefix fastapi-lambda \\ --region ap-southeast-1 # Get alarm state aws cloudwatch describe-alarms \\ --alarm-names fastapi-lambda-5xx \\ --region ap-southeast-1 \\ --query \u0026#39;MetricAlarms[0].StateValue\u0026#39; \\ --output text Step 4: Manual SNS Subscription Configuration Subscribe Email # Subscribe email to SNS topic aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol email \\ --notification-endpoint your-email@example.com \\ --region ap-southeast-1 # Check your email and confirm subscription Subscribe SMS # Subscribe SMS to SNS topic aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol sms \\ --notification-endpoint +1234567890 \\ --region ap-southeast-1 List Subscriptions # List all subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn $SNS_TOPIC_ARN \\ --region ap-southeast-1 Test Notification # Publish test message aws sns publish \\ --topic-arn $SNS_TOPIC_ARN \\ --message \u0026#34;Test alert from FastAPI Lambda\u0026#34; \\ --subject \u0026#34;Test Alert\u0026#34; \\ --region ap-southeast-1 Step 5: Manual Security Configuration Enable ECR Image Scanning # Enable scanning on existing repository aws ecr put-image-scanning-configuration \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 # Verify scanning is enabled aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].imageScanningConfiguration\u0026#39; Configure Secrets Manager Encryption # Create KMS key for Secrets Manager KMS_KEY_ID=$(aws kms create-key \\ --description \u0026#34;FastAPI JWT Secret Key\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;KeyMetadata.KeyId\u0026#39; \\ --output text) # Update secret to use KMS encryption aws secretsmanager update-secret \\ --secret-id fastapi-jwt-secret \\ --kms-key-id $KMS_KEY_ID \\ --region ap-southeast-1 Enable API Gateway Logging # Get API Gateway ID API_ID=$(aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?Name==`fastapi-lambda-http`].ApiId\u0026#39; \\ --output text) # Create CloudWatch log group for API Gateway aws logs create-log-group \\ --log-group-name /aws/apigateway/fastapi-lambda \\ --region ap-southeast-1 # Enable access logging (requires IAM role with logs:CreateLogStream permission) aws apigatewayv2 update-stage \\ --api-id $API_ID \\ --stage-name \u0026#39;$default\u0026#39; \\ --access-log-settings DestinationArn=arn:aws:logs:ap-southeast-1:ACCOUNT_ID:log-group:/aws/apigateway/fastapi-lambda,Format=\u0026#39;{\u0026#34;requestId\u0026#34;:\u0026#34;$context.requestId\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;$context.identity.sourceIp\u0026#34;,\u0026#34;requestTime\u0026#34;:\u0026#34;$context.requestTime\u0026#34;,\u0026#34;httpMethod\u0026#34;:\u0026#34;$context.httpMethod\u0026#34;,\u0026#34;routeKey\u0026#34;:\u0026#34;$context.routeKey\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;$context.status\u0026#34;,\u0026#34;protocol\u0026#34;:\u0026#34;$context.protocol\u0026#34;,\u0026#34;responseLength\u0026#34;:\u0026#34;$context.responseLength\u0026#34;}\u0026#39; \\ --region ap-southeast-1 Step 6: Create Monitoring Dashboard Create CloudWatch Dashboard # Create dashboard JSON cat \u0026gt; dashboard.json \u0026lt;\u0026lt;EOF { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Invocations\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Errors\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Duration (ms)\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Throttles\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Throttles\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34;, \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;yAxis\u0026#34;: { \u0026#34;left\u0026#34;: { \u0026#34;min\u0026#34;: 0 } } } } ] } EOF # Create dashboard aws cloudwatch put-dashboard \\ --dashboard-name FastAPI-Monitoring \\ --dashboard-body file://dashboard.json \\ --region ap-southeast-1 Step 7: Verify Configuration Check All Resources # Verify log group exists aws logs describe-log-groups \\ --log-group-name-prefix /aws/lambda/fastapi-lambda \\ --region ap-southeast-1 # Verify alarms exist aws cloudwatch describe-alarms \\ --alarm-name-prefix fastapi-lambda \\ --region ap-southeast-1 # Verify SNS topic exists aws sns list-topics \\ --region ap-southeast-1 \\ --query \u0026#39;Topics[?contains(TopicArn, `fastapi-lambda-alerts`)]\u0026#39; # Verify SNS subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn $SNS_TOPIC_ARN \\ --region ap-southeast-1 Complete Configuration Script Create configure-monitoring.sh:\n#!/bin/bash set -e REGION=\u0026#34;ap-southeast-1\u0026#34; FUNCTION_NAME=\u0026#34;fastapi-lambda-fn\u0026#34; PROJECT_NAME=\u0026#34;fastapi-lambda\u0026#34; echo \u0026#34;Configuring Monitoring \u0026amp; Security...\u0026#34; # 1. Create CloudWatch Log Group echo \u0026#34;Creating CloudWatch log group...\u0026#34; aws logs create-log-group \\ --log-group-name /aws/lambda/$FUNCTION_NAME \\ --region $REGION 2\u0026gt;/dev/null || echo \u0026#34;Log group already exists\u0026#34; aws logs put-retention-policy \\ --log-group-name /aws/lambda/$FUNCTION_NAME \\ --retention-in-days 14 \\ --region $REGION # 2. Create SNS Topic echo \u0026#34;Creating SNS topic...\u0026#34; SNS_TOPIC_ARN=$(aws sns create-topic \\ --name ${PROJECT_NAME}-alerts \\ --region $REGION \\ --query \u0026#39;TopicArn\u0026#39; \\ --output text) echo \u0026#34;SNS Topic ARN: $SNS_TOPIC_ARN\u0026#34; # 3. Create CloudWatch Alarms echo \u0026#34;Creating CloudWatch alarms...\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name ${PROJECT_NAME}-5xx \\ --alarm-description \u0026#34;Alert when Lambda errors exceed threshold\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=$FUNCTION_NAME \\ --alarm-actions $SNS_TOPIC_ARN \\ --region $REGION # 4. Subscribe Email (replace with your email) read -p \u0026#34;Enter your email for alerts: \u0026#34; EMAIL aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol email \\ --notification-endpoint $EMAIL \\ --region $REGION echo \u0026#34;Monitoring configuration complete!\u0026#34; echo \u0026#34;Check your email to confirm SNS subscription\u0026#34; Run:\nchmod +x configure-monitoring.sh ./configure-monitoring.sh Best Practices Log Retention: Set appropriate retention (14-30 days) to balance cost and compliance Alarm Thresholds: Set based on expected error rates and business requirements Multiple Channels: Use Email, SMS, and Slack for critical alerts Regular Review: Review alarms and adjust thresholds based on actual metrics Cost Monitoring: Set up billing alerts to monitor costs Always test alarms by triggering them manually to ensure notifications work correctly.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building a Secure FastAPI Backend on AWS Lambda with DevSecOps Overview This is a step-by-step deployment guide that will help you deploy a complete FastAPI Backend API running on AWS Lambda (container-based) with a fully automated CI/CD pipeline, integrated security scanning, and Infrastructure as Code using Terraform.\nProject Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\nFollow this guide to deploy a production-ready serverless application with automated security scanning, monitoring, and infrastructure management.\nYou will learn how to:\nBuild a FastAPI application with clean layered architecture (API → Service → Repository) Package FastAPI as a Lambda container image using Mangum adapter Set up CI/CD Integrate security scanning with Semgrep (SAST) and Trivy (container vulnerability scanning) Deploy infrastructure using Terraform modules Store data in DynamoDB tables (products, orders, users) Implement JWT authentication with AWS Secrets Manager Set up monitoring with CloudWatch and SNS alerts Architecture The architecture is divided into three main domains:\nCI/CD Pipeline Domain: GitLab → (Semgrep → Docker Build → Trivy) → ECR → Terraform Deploy Application Domain: API Gateway HTTP API → Lambda (Container) → FastAPI → DynamoDB + Secrets Manager Monitoring Domain: CloudWatch Logs → CloudWatch Alarms → SNS Alerts Request Flow:\nUser Request ↓ API Gateway (HTTP API) ↓ Lambda Function (Container Image from ECR) ↓ Mangum (ASGI Adapter) ↓ FastAPI Application ├── /auth → JWT Authentication ├── /products → Product CRUD Operations └── /orders → Order CRUD Operations ↓ DynamoDB Tables ├── products (product_id, name, price, stock, category) ├── orders (order_id, user_id, products, status) └── users (user_id, email, password_hash) Project Structure Backend-FastAPI-Docker_Build-Pipeline/ ├── backend/ # FastAPI Application │ ├── app/ │ │ ├── api/ │ │ │ ├── routers/ # API endpoints (auth, products, orders) │ │ │ ├── deps.py # Dependency injection │ │ │ └── middleware.py # Correlation ID middleware │ │ ├── core/ # Core configuration │ │ │ ├── config.py # Environment variables │ │ │ ├── security.py # JWT, password hashing │ │ │ └── logging.py # Structured logging │ │ ├── models/ # Pydantic models │ │ │ ├── product.py │ │ │ ├── order.py │ │ │ └── user.py │ │ ├── repositories/ # Data access layer │ │ │ ├── ddb_products.py │ │ │ ├── ddb_orders.py │ │ │ └── ddb_users.py │ │ ├── services/ # Business logic layer │ │ │ ├── product_service.py │ │ │ ├── order_service.py │ │ │ └── auth_service.py │ │ ├── utils/ # Utility functions │ │ │ └── time.py │ │ ├── main.py # FastAPI app entry point │ │ └── lambda_handler.py # Lambda handler (Mangum) │ ├── tests/ # Unit tests │ ├── Dockerfile # Lambda container image │ ├── requirements.txt │ ├── semgrep.yml # Security scanning rules │ └── trivyignore.txt # Trivy ignore list ├── infra/ # Terraform Infrastructure │ ├── main.tf # Main orchestration │ ├── variables.tf │ ├── outputs.tf │ ├── providers.tf │ ├── backend.tf │ └── modules/ │ ├── dynamodb/ # DynamoDB tables │ ├── iam/ # IAM roles \u0026amp; policies │ ├── lambda_container/ # Lambda function │ ├── apigw/ # API Gateway HTTP API │ ├── observability/ # CloudWatch \u0026amp; SNS │ └── route53/ # Custom domain (optional) └── pipeline/ # CI/CD Pipeline ├── buildspec-build.yml # Build stage (Semgrep, Docker, Trivy) ├── buildspec-deploy.yml # Deploy stage (Terraform) ├── codepipeline.tf # CodePipeline configuration └── README.md Content Workshop Overview Prerequisites Setting up CI/CD Pipeline Building FastAPI Backend Monitoring \u0026amp; Observability Clean up Resources AWS Services Used Category Services Compute AWS Lambda (Container Image) API API Gateway HTTP API (v2) Database Amazon DynamoDB (3 tables) Security AWS Secrets Manager, IAM CI/CD Amazon ECR Monitoring Amazon CloudWatch (Logs, Metrics, Alarms), SNS DNS Amazon Route 53 (optional) IaC Terraform Security Scanning Semgrep (SAST), Trivy (Container Scan) Estimated Time \u0026amp; Cost Item Details Duration 4-5 hours Level Intermediate to Advanced Cost ~$5-10 (if cleaned up after workshop) "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand advanced AWS services for data migration, security, and automation. Learn how to secure, monitor, and manage AWS resources using GuardDuty, macie, and secrets manager. Gain hands-on experience with lambda and EventBridge for automated EC2 management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS data migration service - Practice: + Setup lab environment + Create a replication instance + Configure source and target endpoints + Create and run a migration task 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon macie\n- Practice: + Create custom data identifiers + Create and run a macie job to analyze sensitive data 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS secrets manager - Practice: + Store RDS credential values securely through AWS secrets manager + Write a bash script to connect to the RDS instance using stored secrets 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon GuardDuty - Learn how to use EventBridge and lambda to automatically isolate compromised EC2 instances when suspicious activity is detected 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about AWS lambda - Practice: + Create a lambda function to start an EC2 instance through EventBridge + Create a Lambda function to stop an EC2 instance through EventBridge 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Got familiar with AWS data migration service and learned how to:\nCreate a replication instance Configure source and target endpoints Create and run migration tasks Being able to detect and classify sensitive data through Amazon macie\nUnderstood how to use AWS secrets manager and practiced:\nStoring RDS credentials securely using secrets manager Writing a bash script to connect to RDS Got familiar with Amazon GuardDuty and enhance security using EventBridge and lambda to automatically protect the system\nLearned how to use AWS lambda and practiced:\nCreate a lambda function to start EC2 instances via EventBridge Create a lambda function to stop EC2 instances via EventBridge. \u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.6-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources Clean up all AWS resources created during the workshop to avoid unnecessary costs.\nQuick Cleanup The easiest way is using Terraform:\ncd Backend-FastAPI-Docker_Build-Pipeline/infra terraform destroy This destroys:\nLambda function API Gateway HTTP API DynamoDB tables IAM roles CloudWatch log groups SNS topics CloudWatch alarms Manual Cleanup Steps If Terraform destroy doesn\u0026rsquo;t work or you need to clean up manually:\n1. Delete ECR Repository # Delete all images first aws ecr list-images \\ --repository-name fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;imageIds[*]\u0026#39; \\ --output json \u0026gt; image-ids.json aws ecr batch-delete-image \\ --repository-name fastapi-lambda \\ --image-ids file://image-ids.json \\ --region ap-southeast-1 # Delete repository aws ecr delete-repository \\ --repository-name fastapi-lambda \\ --force \\ --region ap-southeast-1 2. Delete Secrets Manager Secret aws secretsmanager delete-secret \\ --secret-id fastapi-jwt-secret \\ --force-delete-without-recovery \\ --region ap-southeast-1 3. Delete CloudWatch Log Groups aws logs delete-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 4. Delete SNS Subscriptions # List subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn arn:aws:sns:ap-southeast-1:123456789012:fastapi-lambda-alerts \\ --region ap-southeast-1 # Unsubscribe (replace with actual subscription ARN) aws sns unsubscribe \\ --subscription-arn arn:aws:sns:ap-southeast-1:123456789012:fastapi-lambda-alerts:subscription-id \\ --region ap-southeast-1 Verify Deletion Check Lambda Functions aws lambda list-functions \\ --region ap-southeast-1 \\ --query \u0026#39;Functions[?contains(FunctionName, `fastapi`)].FunctionName\u0026#39; Should return empty.\nCheck API Gateway aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?contains(Name, `fastapi`)].Name\u0026#39; Check DynamoDB Tables aws dynamodb list-tables \\ --region ap-southeast-1 \\ --query \u0026#39;TableNames[?contains(@, `products`) || contains(@, `orders`) || contains(@, `users`)]\u0026#39; Check ECR Repositories aws ecr describe-repositories \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[?contains(repositoryName, `fastapi`)].repositoryName\u0026#39; Complete Cleanup Script Create cleanup.sh:\n#!/bin/bash REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;Destroying Terraform infrastructure...\u0026#34; cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform destroy -auto-approve echo \u0026#34;Deleting ECR repository...\u0026#34; aws ecr delete-repository \\ --repository-name fastapi-lambda \\ --force \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Deleting Secrets Manager secret...\u0026#34; aws secretsmanager delete-secret \\ --secret-id fastapi-jwt-secret \\ --force-delete-without-recovery \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Deleting CloudWatch log group...\u0026#34; aws logs delete-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Cleanup completed!\u0026#34; Run:\nchmod +x cleanup.sh ./cleanup.sh Cost Verification After cleanup:\nNavigate to AWS Cost Explorer Check recent charges Verify no resources are running Expected costs after cleanup: $0 (except free tier usage)\nCommon Issues Issue: Terraform destroy fails Solution: Delete resources manually in order, or use terraform destroy -target for specific resources.\nIssue: ECR repository not empty Solution: Delete all images first, then delete repository.\nIssue: DynamoDB table deletion takes time Solution: Wait for deletion to complete. Check status with aws dynamodb describe-table.\nBest Practices Always use Terraform destroy when possible Verify deletion to ensure no orphaned resources Set up billing alerts to monitor unexpected charges Use tags to identify workshop resources Document manual steps for resources not managed by Terraform Make sure to delete all resources to avoid incurring costs. DynamoDB tables and Lambda functions can accumulate charges if left running.\n"
},
{
	"uri": "http://localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam from 08-09-2025 to 08-12-2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the deployment of a secure serverless application on AWS, featuring a fully automated CI/CD pipeline with integrated security scanning, infrastructure as code, and centralized monitoring. Through this project, I improved my skills in cloud architecture design, DevOps automation, security analysis, infrastructure provisioning with Terraform, system integration, and professional communication.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Develop more effective problem-solving skills by proactively seeking solutions and learning from challenges encountered during work. Enhance communication skills in both daily interactions and professional settings, including the ability to clearly present ideas, report work, and handle situations with confidence. Be more proactive in sharing ideas, reporting progress, and collaborating with colleagues to improve teamwork. Actively contribute constructive feedback and propose new initiatives to increase team effectiveness and innovation. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Visualize and monitor system metrics using grafana Manage and automate AWS operations with systems manager Implement infrastructure as code using CloudFormation and Terraform Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Grafana fundamentals - Practice: + Install grafana + Visualize metrics in dashborads 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn how systems manager centralizes operational tasks\n- Practice: + Use patch manager to automate OS and software patching + Execute remote commands on multiple EC2 instances using run command 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - Understand YAML syntax and the structure of an AWS CloudFormation template - Write a basic template to provision an EC2 instance 10/22/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Terraform fundamentals for automating AWS infrastructure provisioning - Practice: + Write a main.tf file to deploy basic AWS resources + Use variables to protect sensitive data at runtime + Create multiple workspaces to separate environments 10/23/2025 10/26/2025 https://youtu.be/7xngnjfIlK4?si=VhwAI1v4UCCSJQZF 6 - Learn about the basics of continuous integration and deployment\n- Practice: + Use gitHub actions to automate the build and deployment process 10/26/2025 10/26/2025 https://www.youtube.com/watch?v=ZKaDy0mNHGs Week 7 Achievements: Got familiar with grafana fundamentals and core features and know how to:\nInstalled and configured grafana server Created dashboards to visualize system metrics in real time Understood how systems manager centralizes operational management on AWS\nUnderstood the concept of Infrastructure as Code(Terraform and CloudFormation):\nLearned YAML syntax and CloudFormation template structure Created a simple CloudFormation template to provision an EC2 instance Understood key components: parameters, resources, and outputs Studied Terraform basics for automating AWS infrastructure deployment Wrote a main.tf file to launch core AWS resources Used variables to manage sensitive data securely during runtime \u0026hellip; Gained understanding of continuous integration and deployment concepts\nBuilt an automated workflow using gitHub actions for build and deploy processes\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is professional and friendly. The open atmosphere makes it easy for me to communicate and learn from others. Clear workflows and sufficient supporting tools help me maintain productivity and stay focused on my tasks.\n2. Support from Mentor / Team Admin\nThe mentor team is very supportive of everyone. Whenever I encounter difficulties, I always receive detailed guidance and suggestions on how to resolve the issues. The team encourages me to research proactively, which has helped me improve my self-learning and problem-solving skills.\n3. Relevance of Work to Academic Major\nThe tasks assigned are aligned with my academic major, allowing me to apply theoretical knowledge to real-world projects. I also have the opportunity to explore new technologies that are not covered in the university curriculum.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I gained a great deal of knowledge about Cloud, including deployment processes, resource management, and essential services within the cloud ecosystem. These skills have helped me build a solid foundation for my future career path.\n5. Company Culture \u0026amp; Team Spirit\nDuring my internship, the company organized many events where I had the chance to listen to real experiences and insights shared by senior employees in the industry. These sharing sessions provided valuable knowledge and helped me better understand potential career paths.\n6. Internship Policies / Benefits\nThe company’s policies are clear and were communicated from the beginning. I always understood the expectations throughout the internship period. The company provided a supportive environment, giving me access to the necessary resources to complete my tasks efficiently.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with the opportunity to participate in events about Cloud, GenAI, and DevOps, and to listen to valuable sharing from experienced professionals. These experiences helped broaden my knowledge and gave me new perspectives for my future career.\nWhat do you think the company should improve for future interns?\nThe company could organize more team bonding activities or workshops for interns to connect and learn from each other. More structured onboarding or training at the beginning would also help new interns adapt faster.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would recommend this internship to my friends because the environment is friendly, the mentors are dedicated, and there are many opportunities to learn and grow both professionally and personally.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nRegular one-on-one check-ins or feedback meetings (weekly or bi-weekly) would allow interns to better understand their progress and identify areas for improvement.\nWould you like to continue this program in the future?\nYes, I would be happy to continue if given the opportunity, as I feel there is still much to learn and contribute.\nAny other comments (free sharing):\nThank you to everyone in the team for your support and guidance throughout my internship. I truly appreciate the experience and hope to stay connected in the future.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Reinforce AWS architecture and core services Learn about Docker Build CI/CD pipelines using Docker and gitHub actions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review AWS architecture + Study fundamental AWS architecture concepts + Understand AWS security architecture best practices + Learn high availability and high-performance architecture design + Explore cost optimization strategies - Review AWS core services : EC2, VPC, IAM, S3, RDS, CloudWatch, etc 10/27/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Docker - Practice: + Use essential Docker commands + Run a simple web application + Write a Dockerfile to build custom images 10/28/2025 10/28/2025 https://www.youtube.com/watch?v=Y3zqsFpUzMk\u0026list=PLncHg6Kn2JT4kLKJ_7uy0x4AdNrCHbe0n 4 - Learn about CI/CD with Docker and gitHub actions - Practice: + Write a gitHub actions workflow to build and test code when pushed + Write and optimize Dockerfile for CI/CD pipelines - Configure gitHub actions to automatically: + Build the Docker image + Deploy the application when new code is pushed 10/29/2025 10/29/2025 https://www.youtube.com/watch?v=Y3zqsFpUzMk\u0026list=PLncHg6Kn2JT4kLKJ_7uy0x4AdNrCHbe0n 5 - Work with Amazon ECR and application deployment - Practice: + Create an EC2 instance to run Docker containers + Create and configure an RDS instance for database usage + Build Docker image and run container locally before deployment + Create an ECR repository and push Docker images to ECR 10/30/2025 10/30/2025 https://www.youtube.com/watch?v=Y3zqsFpUzMk\u0026list=PLncHg6Kn2JT4kLKJ_7uy0x4AdNrCHbe0n 6 - Learn about Amazon ECS - Practice: + Create an ECS cluster + Create a task definition based on the image stored in ECR + Deploy the application using ECS service + Test and verify successful deployment 10/31/2025 10/31/2025 https://youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2\u0026si=baiJ4O6XbfmMEFvH Week 8 Achievements: Reinforced AWS Core architecture and services\nThe fundamental cloud architecture model Reviewed core AWS services such as EC2, S3, IAM, RDS, and CloudWatch Identified the role of each service in building scalable applications Got familiar with Docker usage\nPracticed common Docker commands (build, run, ps, logs, exec) Learned how to containerize a simple web application using Docker Wrote a basic Dockerfile and understood image layer structure Got Hands-On with CI/CD using Docker and gitHub actions\nSet up a GitHub Actions workflow to automate application building Configured GitHub Actions pipeline to build Docker images automatically Reviewed how CI/CD helps deploy updated versions on every code push Get Experience Working With Amazon ECR and Deployment\nLaunched and configured an EC2 instance to run Docker containers Deployed a web application container and tested successful operation Created an ECR repository and pushed Docker images to it Understood how ECS Cluster organizes computing resources\nLearned the purpose of task definitions and how they describe container settings\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Deploy containers on ECS using EC2 and Fargate Understand ECS networking modes Implement load balancing and path-based routing with ALB Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice running containers on ECS with EC2: + Create an ECR repository + Build and upload the container image to ECR + Create an ECS cluster + Define a task definition using EC2 + Create a service to launch the task + Verify the deployment is successful 11/03/2025 11/03/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 3 - Practice running containers on ECS with Fargate: + Push the container image to Docker Hub + Create a task definition for Fargate + Create a service and launch the task + Scale the service by increasing the desired task count + Verify the deployment is successful 11/04/2025 11/04/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 4 - Learn about ECS network modes - Understand and compare the following network modes: + bridge + host + awsvpc 11/05/2025 11/05/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 5 - Deploy containers with Application Load Balancer: + Create an Application Load Balancer + Create a target group + Create a service integrated with the ALB + Test and verify that traffic is routed correctly 11/06/2025 11/06/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 6 - Run multiple services behind the same ALB: + Create separate target groups for different paths (/email, /user) + Create services mapped to each target group + Test the deployment to ensure correct path routing 11/07/2025 11/07/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 Week 9 Achievements: Practiced running containerized applications on Amazon ECS:\nBuilt and pushed container images to both ECR and Docker Hub Created ECS clusters using both EC2 and Fargate launch types Created task definitions and deployed services Scaled services by adjusting the desired task count Verified successful application deployment and accessibility Learned about ECS network modes:\nCompared the characteristics and use cases of bridge, host, and awsvpc modes Understood how awsvpc mode assigns ENIs and improves service isolation Gained hands-on experience deploying containers with Application Load Balancer (ALB)\nCreated an ALB and configured target groups Integrated ECS services with the ALB Tested routing and confirmed traffic forwarding worked correctly Deployed multiple services behind a single ALB\nCreated multiple target groups for different application paths (/email, /user, /) Mapped ECS services to the corresponding target groups Tested and verified correct path-based routing behavior "
},
{
	"uri": "http://localhost:1313/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Reinforce AWS DevOps workflow using CodeBuild, CodePipeline, ECR, and ECS Practice building and deploying APIs using API Gateway and Lambda Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Use CodeBuild to automatically push image to ECR + Create a CodeBuild project and connect it to GitHub + Create the required IAM roles for CodeBuild + Write a buildspec.yml file to build and push a Docker image to Amazon ECR + Run a test build to verify that the image is successfully pushed to ECR 11/10/2025 11/10/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 3 - Build a CI/CD Pipeline Using AWS CodePipeline - Practice: + Create a CodePipeline pipeline that listens to GitHub pushes + Create required IAM roles for CodePipeline and deployment actions + Deploy container updates to ECS automatically + Push code to GitHub and verify that the ECS service updates successfully 11/11/2025 11/11/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 4 - Learn about Amazon API Gateway + Create API resources + Create and integrate Lambda functions with API Gateway + Create methods and test invoking the Lambda function 11/12/2025 11/12/2025 https://youtube.com/playlist?list=PL4NoNM0L1m71akbZ8uzEOupMULmMpR-s7\u0026si=kSCnIDdngdWk4C6D 5 - Practice Input Validation in API Gateway - Practice: + Create API Gateway models to validate request headers and body + Add mapping templates for the method request + Test validation logic using the API Gateway console 11/13/2025 11/13/2025 https://youtube.com/playlist?list=PL4NoNM0L1m71akbZ8uzEOupMULmMpR-s7\u0026si=kSCnIDdngdWk4C6D 6 - Use stage variables for multi-environment deployment - Practice: + Create a Lambda function and versions + Create aliases and map them to different stages + Add stage variables and reference them in Lambda integration + Test and verify successful deployment 11/14/2025 11/14/2025 https://youtube.com/playlist?list=PL4NoNM0L1m71akbZ8uzEOupMULmMpR-s7\u0026si=kSCnIDdngdWk4C6D Week 10 Achievements: Strengthened AWS DevOps practices with CodeBuild, CodePipeline, ECR, and ECS\nCreated a CodeBuild project Wrote and tested a complete buildspec.yml to build and push images to Amazon ECR Set up a CodePipeline pipeline triggered on GitHub pushes Integrated source, build, deploy stages to automate container updates on ECS Tested end-to-end deployment and confirmed ECS service updates after each push Got familiar with API Gateway architecture\nCreated resources, methods, and Lambda integrations Built a simple backend API using API Gateway and Lambda Tested method responses and Lambda invocation Implemented request validation in API Gateway\nCreated request models to validate headers and request body Added and configured mapping templates for method requests Verified validation logic using the API Gateway console Practiced multi-environment deployment using stage variables\nCreated Lambda versions and aliases for different environments Configured stage variables to dynamically select Lambda versions Tested multi-stage behavior for dev, staging, and prod "
},
{
	"uri": "http://localhost:1313/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Practice containerized Node.js application deployment using Docker and Docker Compose Practice Infrastructure as Code (IaC) with Terraform to deploy AWS services Learn about prompt engineering Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create a small Node.js project running in Docker + Create a Docker network + Run a database container + Run an app container and connect it to the database + Delete the app container and create it again + Check that data in the database persists 11/17/2025 11/17/2025 https://docs.docker.com/get-started/workshop/ 3 - Deploy Node.js app using Docker Compose + Write a docker-compose.yml file to define app and database services + Run docker-compose up and test deployment 11/18/2025 11/18/2025 https://docs.docker.com/get-started/workshop/ 4 - Practice Infrastructure as Code (IaC) with Terraform + Write scripts to create AWS VPC, subnets, and security groups + Create NAT gateway, ALB, target groups + Create RDS instance and IAM policies + Create ECS cluster, task definitions, and service 11/19/2025 11/21/2025 5 - Deploy AWS services using Terraform + Apply Terraform scripts to create AWS resources + Verify that resources are created correctly + Check that ECS services and RDS are running stably 11/21/2025 11/21/2025 6 - Learn prompt engineering techniques 11/22/2025 11/22/2025 https://youtu.be/_ZvnD73m40o?si=ud6xi6-S2NsvA7dr Week 11 Achievements: Successfully containerized a Node.js application using Docker\nCreated a Docker network, database container, and app container Verified data persistence after recreating the app container Tested inter-container connectivity Deployed Node.js app with Docker Compose\nWrote a complete docker-compose.yml file including app and database services Ran docker-compose up and verified application functionality Practiced Infrastructure as Code (IaC) using Terraform\nCreated VPC, subnets, security groups, NAT gateway, ALB, and target groups Deployed RDS instance and configured IAM policies Created ECS cluster, task definitions, and services Successfully deployed AWS infrastructure using Terraform\nVerified that all AWS resources were created correctly Confirmed ECS services and RDS instance were running stably "
},
{
	"uri": "http://localhost:1313/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Complete all remaining tasks of the final project, including WAF configuration Verify full system functionality and finalize project documentation Learn Kubernetes fundamentals and practice deploying a Kubernetes cluster on on-premise virtual machines Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Complete all remaining tasks in the final project + Review incomplete configuration items + Ensure all services are fully deployed 11/24/2025 11/24/2025 3 - Configure AWS WAF for the project + Attach WAF to the CloudFront distribution + Create and apply WAF rules 11/25/2025 11/25/2025 4 - Verify project functionality - Summarize and complete project documentation 11/26/2025 11/27/2025 5 - Learn about Kubernetes fundamentals + Master node architecture + Worker node architecture 11/28/2025 11/28/2025 https://devopsedu.vn/courses/khoa-hoc-kubenetes-thuc-te/ 6 - Practice Kubernetes deployment on on-premise VMs 11/29/2025 11/29/2025 https://devopsedu.vn/courses/khoa-hoc-kubenetes-thuc-te/ Week 12 Achievements: Completed the final project, configuring WAF Verified full system functionality and ensured all services operated without errors Finalized project documentation and completed all required reports Learned about Kubernetes fundamentals and practiced deploying a Kubernetes cluster on on-premise virtual machines "
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]