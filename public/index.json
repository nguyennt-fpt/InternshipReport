[
{
	"uri": "http://localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: Vietnam Cloud Day 2025 Event Objectives Share real-world experiences from enterprises that have migrated and modernized on-premises workloads with AWS Introduce AI-powered Amazon Q developer to accelerate the SDLC Discuss application modernization strategies that drive digital business transformation Provide guidance on modernizing VMware estates with AI-driven cloud Present approaches to achieving AWS security at scale Speakers Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Hung Hoang - Customer Solutions Manager, AWS Taiki Dang - Solutions Architect, AWS Key Highlights Large-scale migration and modernization with AWS Lessons from thousands of enterprises: migrate and modernize simultaneously Best practices, proven mental models, and AWS migration accelerators Modernizing applications with generative AI (Amazon Q Developer) Q Developer enhances the entire SDLC across AWS console, IDE, CLI, and DevSecOps platforms Automates code generation, documentation to improved quality and maintainability Panel Discussion Experts from OCB, LPBank Securities, and Ninety Eight shared their perspectives Focused on AI-powered modernization, event-driven approaches, and business transformation Emphasis: AI reduces coupling, accelerates development Transforming VMware with AI-driven cloud modernization How AWS transform enables fast, safe, and cost-efficient VMware migrations Step-by-step transformation with human-in-the-loop validation AWS security at scale End-to-end approach: identify, prevent, detect, respond, remediate Security at the edge: layered security Generative AI strengthens security analysis and automates response Key Takeaways Migration and modernization Simply migrating to the cloud is not enough Migration and modernization: go hand-in-hand for sustainable digital transformation AI-powered tools like Q Developer accelerate the SDLC AI can assist in code generation, test creation, documentation, and optimization This improves code quality, shortens development cycles, and reduces operational costs Developers can focus more on innovation instead of repetitive tasks VMware to AWS A step-by-step migration strategy is needed, from lift-and-shift to gradual modernization Downtime-aware migration is critical to avoid business disruption Security Security cannot be an afterthought, it must be integrated throughout the development lifecycle Security by design ensures identity, prevention, detection, and response AI can enhance log analysis, threat detection, and automated remediation Applying to Work Use AI tools like Amazon Q Developer to automate refactoring and testing Embed security from the start with AI-driven detection and response Use AWS Security Hub to centralize findings, prioritize risks, and ensure compliance Event Experience Attending the “Migration \u0026amp; Modernization” track was extremely valuable, giving me a comprehensive perspective on how organizations can migrate, modernize, and secure workloads using AWS services and AI-powered tools. Key experiences included:\nLearning from highly skilled speakers AWS experts and industry leaders shared best practices in migration and modernization at scale Real-world case studies (Techcombank) showed how enterprises build strong cloud foundations and roadmaps for digital transformation Leveraging modern tools Explored Q Developer as an intelligent collaborator across the SDLC to accelerate migration and application refactoring Learned how AI can enhance cloud security by automating analysis, detection, and remediation Saw how AWS Security Hub provides centralized visibility and compliance management across workloads Networking and discussions Had the opportunity to meet new peers who share the same interest and passion for migration and modernization Exchanged insights and experiences with others in the field, fostering mutual learning and collaboration Lessons learned Migration and modernization must go hand-in-hand to achieve sustainable digital transformation Event-driven design reduces coupling while improving scalability and resilience AI tools like Q Developer can cut costs and boost productivity when integrated into workflows Security must be embedded from the start, supported by services like AWS Security Hub for compliance and risk management Overall, the event enriched my technical expertise while inspiring a new perspective on application design, modernization strategies, and effective cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Student Information: Full Name: Nguyễn Thành Nguyên\nPhone Number: 0935457152\nEmail: nguyen.fridayed@gmail.com\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Build new connections and explore course materials. Understand AWS and its advantages over traditional models. Create and secure an AWS account with MFA and IAM users. Learn cost management and practice creating budgets. Gain hands-on experience with IAM, VPC, and VPN setup and troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Make new friends and connections - Explore course materials and know what to do 09/08/2025 09/08/2025 3 - Learn what AWS is and its advantages over traditional models + Cost + Scalability and elasticity + Agility and speed + Reliability and availability 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create an AWS account - Practice: + Create an account and complete tasks to receive free credits + Enable MFA for stronger security + Create IAM users for better account protection 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about cost management + Learn the 4 types of budgets and their differences - Practice: + Create each type of budget 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about IAM groups and IAM users - Learn how to use roles and their benefits - Learn about firewalls in VPC - How to create a VPC and related components - Site-to-site VPN connection - Practice: + Create an IAM group + Create an IAM user and add it to the group + Create a role and assign it to the user + Create VPC, subnets, and an internet gateway + Create route tables, NAT gateway, and security groups + Create virtual private gateway, customer gateway, and VPN connection + VPN troubleshooting 09/11/2025 09/13/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Built new connections and explored the course materials to understand the learning path.\nUnderstood what AWS is and its key advantages over traditional models:\nCost Scalability and elasticity Agility and speed Reliability and availability \u0026hellip; Successfully created and configured an AWS free tier account, enabled MFA for stronger security, and created IAM users for better account protection.\nLearned cost management concepts and practiced creating the four types of budgets.\nLearned about IAM users, groups, and roles, along with their benefits.\nGained knowledge of VPC, its components, and firewalls in the networking environment.\nPracticed hands-on by creating and configuring:\nIAM groups, users, and roles VPC, subnets, and internet gateway Route tables, NAT gateway, and security groups Virtual private gateway, customer gateway, and VPN connection \u0026hellip; Performed VPN troubleshooting and validated connectivity.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Exploring AWS networking, multi-VPC connectivity, and application deployment\nWeek 3: Practiced AWS core services for monitoring, scaling, and resource management\nWeek 4: Enhanced skills in deploying and managing core AWS services such as DynamoDB, CloudFront, and WorkSpaces\nWeek 5: Strengthened AWS security and governance through IAM, KMS, and CloudTrail\nWeek 6: Improved skills in data migration, security automation, and EC2 management using DMS, Macie, GuardDuty, Secrets Manager, and Lambda\nWeek 7: Exploring AWS Monitoring, Automation, and Infrastructure as Code\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, â€œGetting Started with Healthcare Data Lakes: Diving into Amazon Cognitoâ€, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the â€œpub/sub hub.â€\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function â†’ ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda â€œtriggerâ€ subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 â†’ JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn how to manage and connect EC2 instances using session manager. Practice VPC peering and apply ACL and security group rules for cross-VPC communication. Understand the difference between VPC peering and transit gateway. Explore hybrid DNS with route 53 resolver and AWS managed microsoft AD. Deploy applications on EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about session manager - Practice: + Create VPC endpoint for session manager + Create S3 bucket to store session logs 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about VPC peering - How to apply rules in ACLs and security groups for cross-VPC traffic - Practice: + Create 2 VPCs + Set up VPC peering + Apply rules in ACLs and security groups for cross-VPC traffic 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about transit gateway - Understand the different between VPC peering and transit gateway - Practice: + Create 4 VPCs + Create transit gateway and attachments + Configure transit gateway route table and VPC route tables 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about hybrid DNS with route 53 resolver - Learn about AWS directory service and AWS managed microsoft AD - Practice: + Set up network infrastructure using AWS cloudFormation + Configure inbound/outbound endpoints for DNS resolution 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about application deployment on EC2 (Windows and Linux) - Practice: + Deploying databases and web services via CLI on linux + Deploying databases and web services via GUI and CLI on windows 09/18/2025 09/20/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Successfully managed and connected to EC2 instances in both public and private subnets using AWS session manager.\nEstablished VPC peering between two VPCs and applied proper ACL and security group rules to enable cross-VPC communication.\nValidated and troubleshooted VPC peering connectivity using reachability analyzer to identify and fix route table and security group issues.\nBuilt multi-VPC connectivity with AWS transit gateway, including:\nTransit gateway attachments Configured transit gateway route table Configured VPC route table \u0026hellip; Implemented Hybrid DNS resolution using Route 53 Resolver.\nSet up AWS Managed Microsoft AD through AWS Directory Service and deployed network infrastructure using CloudFormation.\nDeployed applications on EC2 instances (Windows and Linux) using both CLI and GUI methods.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: AI-Driven Development Life Cycle: Reimagining Software Engineering Event Objectives The role of AI in modern software development, from AI-assisted to AI-managed workflows Explore how AI is transforming the development lifecycle with a human-centric approach Identify challenges and opportunities when integrating AI into end-to-end development processes Learn how AI-DLC can accelerate delivery, improve quality, and respond faster to market needs Speakers Toan Huynh - Senior, AWS My Nguyen - Senior, AWS Key Highlights AI shaping software development AI helps developers write code faster and handle multiple tasks simultaneously Current AI can complete end-to-end development tasks, but human-in-the-loop is necessary for validation and control Impact: reduces time from idea to market and improves responsiveness to quality requirements Challenges with AI in development Scaling large projects remains challenging: limited control and potential code quality issues Multi-step tasks require strategic context refresh and careful planning Current AI models: AI-assisted (tasks divided into small units) vs AI-managed (still emerging) AI-Driven lifecycle (AI-DLC) AI acts as a collaborator, not the sole decision-maker Core workflow: plan, seek clarification, provide clarification, implement Lifecycle stages: inception, construction, operation Spec-driven development ensures clear input/output, context, and documentation at every stage Kiro Agent hooks and advanced context management help AI understand complex tasks How to use Kiro to optimize work performance Key Takeaways Human-Centric AI collaboration AI accelerates tasks but does not replace humans, human validation is essential Checkpoints ensure AI output meets expectations and quality standards Structured AI workflow Clear input/output, task division, and context management improve efficiency Interactive approach: AI asks clarifying questions, humans provide guidance Efficiency and quality Faster delivery, reduced coding errors, and better alignment with market requirements Spec-driven development provides process control and ensures consistency Scalability Challenges Managing context and multi-step problems is critical in large-scale projects AI-DLC provides a structured methodology that allows scalable development while preserving control and quality standards Applying to Work Use AI to assist with coding, testing, and deployment for faster delivery Implement AI-DLC workflow for large projects to maintain quality and control Integrate human-in-the-loop validation before deploying AI outputs Apply spec-driven development to ensure clarity of inputs, outputs, and documentation Event Experience Attending the AI-Driven Development Lifecycle event was extremely valuable, providing me with a comprehensive understanding of leveraging AI to accelerate development, optimize workflows, and maintain quality throughout the software lifecycle. Key experiences included:\nLearning from highly skilled speakers Observed how software seniors integrate AI into real development workflows. Learned how human-in-the-loop ensures output quality and control. Understood challenges in scaling projects and managing context effectively. Recognized the importance of documentation and spec-driven development in practical settings. Observing AI Task Demonstrations Saw demos of AI assisting in code generation, testing, and deployment. Experienced AI asking for clarification and humans providing feedback to improve outcomes. Lessons learned AI is a collaborator, not a replacement; human-centric workflow is critical. Clear process, task division, and context management improve efficiency and reduce errors. Interactive workflow ensures continuous improvement and high-quality output. Practical Insights for Future Work AI-DLC methodology can be applied to personal or team projects to improve speed and quality. Experience highlights the need for a structured framework to integrate AI in development. Understanding AI’s potential in large-scale projects with proper management framework. Overall, the event not only provided practical insights into AI-driven development but also helped me rethink software design, end-to-end development processes, and effective human-AI collaboration.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "IoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement Whatâ€™s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two partsâ€”setting up weather edge stations and building the weather platformâ€”each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Report: AI/ML/GenAI on AWS Event Objectives Introduce an overview of AI/ML and GenAI on AWS.\nHelp participants understand and get hands-on experience with SageMaker and Bedrock.\nGuide building practical GenAI solutions such as RAG and chatbots.\nSpeakers AWS GenAI Specialist Team AWS Solutions Architects Guest AI Practitioners from industry Key Highlights Prompting techniques for LLMs Zero shot prompting: the model infers without examples Few shot prompting: using a few examples to improve accuracy and consistency Chain of thought prompting: guides the model to think step-by-step to improve reasoning Retrieval Augmented Generation (RAG) Overview of RAG mechanism RAG use cases: enterprise chatbots, document search, report analysis assistants, technical support How to build a RAG pipeline Embedding and Amazon Titan Embeddings Embedding: vector representation of data to help the model understand semantics Applications of embeddings in search, clustering, semantic similarity, and RAG Amazon Titan Embedding: high accuracy, optimized for RAG, supports multi-language Amazon Bedrock Bedrock is AWS’s fully managed GenAI platform Supports multiple foundation models and Titan models Provides a unified API and enterprise-level data security Built-in features for Guardrails, RAG, Memory, and Agents Agentic AI AI is evolving from passive responses to agentic AI, capable of: Planning Goal-oriented actions Tool calling Multi-system communication Bedrock Agent Core Agent-centric architecture to run multiple agents at scale Supports: Routing between agents Tool calling orchestration Retrieval integration Runtime guardrails Agent Core service enabling agents at scale Allows deployment of agents with: High availability Scalable worker orchestration Built-in monitoring and governance Easy integration with enterprise systems Key Takeaways Prompting Zero shot is suitable for simple tasks Few shot improves accuracy in complex tasks Chain of thought helps the model explain and reason better for logical problems Bedrock and Agentic AI Bedrock enables enterprises to deploy GenAI without managing infrastructure Agentic AI is the future for enterprise automation Bedrock Agent Core simplifies creating, managing, and scaling AI agents Applying to Work Use Zero shot, Few shot, and Chain of thought to improve prompting quality Event Experience Attending the event provided a fresh perspective on building modern AI systems, from prompting techniques to RAG and agents.\nLearning from experts Experts explained the architecture of prompting, embedding, and RAG clearly Real-world case studies demonstrated enterprises using GenAI for operations and automation Hands-on demonstrations Live demo of building a Generative AI chatbot using Bedrock Lessons learned Good prompting determines output quality Agentic AI is essential for advanced automation Bedrock Agent Core enables building strong, secure, and scalable agents The event broadened my perspective on GenAI, especially the usefulness of intelligent AI agents and applying them to enterprise workflows.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Practice AWS core services including tags, resource groups, cloudwatch, and auto scaling. Deploy wordpress on lightsail and manage resources with AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about tags and resource groups - Practice: + Add tags to resources + Create and manage a resource group 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Explore Amazon cloudwatch - Understand cloudwatch metrics - Practice: + View and analyze EC2 logs in cloudwatch + Create cloudwatch alarms + \u0026hellip; 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about auto scaling groups - Practice: + Set up a lab environment + Configure a load balancer + Create and test an auto scaling group 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon lightsail - Get familiar with wordpress - Practice: + Launch a wordpress instance + deploy and manage wordpress + Create an instance snapshot in Amazon lightsail + \u0026hellip; 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about the AWS CLI - Explore AWS cloud9 - Practice: + Install and configure the CLI + Create and manage resources using CLI commands 09/26/2025 09/27/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood the concept of tags and resource groups in AWS:\nLearned how tags support cost allocation and environment grouping Gained the ability to create and manage resource groups for centralized management Understood the role of cloudwatch in monitoring AWS resources and applications.\nBecame familiar with auto scaling group and how to configure and test auto scaling policies to automatically adjust capacity.\nUnderstood how lightsail simplifies application deployment:\nLearned how to deploy and manage WordPress instances Practiced creating snapshots for backup and recovery \u0026hellip; Learned to use CLI commands to manage resources as an alternative to the web console.\nBecame familiar with cloud9 as a cloud-based IDE.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devicesâ€¦), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: DevOps on AWS Event Objectives Present the CI/CD ecosystem on AWS using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Introduce Infrastructure as Code (IaC) with CloudFormation and AWS CDK Explain container deployment using Docker, ECR, ECS, EKS, and App Runner Demonstrate how to set up Monitoring and Observability with CloudWatch and AWS X-Ray Share DevOps best practices, case studies, and career pathways Speakers Bao Huynh - AWS Community Builders Thinh Nguyen - AWS Community Builders Vi Tran - AWS Community Builders Key Highlights AWS CI/CD Pipeline Source Control: CodeCommit Build \u0026amp; Test: CodeBuild pipelines with unit and integration tests Deployment: CodeDeploy supporting Blue/Green, Canary, and Rolling updates Automation: CodePipeline orchestrating the full CI/CD workflow Infrastructure as Code (IaC) AWS CloudFormation: templates, stack lifecycle, drift detection AWS CDK: constructs, reusable patterns, multi-language support Demo: deploying resources using both CloudFormation and CDK Discussion: choosing the right IaC tool based on project requirements Container Services on AWS Docker fundamentals: containerization and microservices Amazon ECR: image scanning and lifecycle policies Amazon ECS \u0026amp; EKS: orchestration, scaling, service discovery AWS App Runner: simplified container deployment without infrastructure management Monitoring and Observability CloudWatch: metrics, logs, alarms, dashboards AWS X-Ray: distributed tracing and end-to-end performance analysis Key Learnings DevOps accelerates deployment cycles Automated CI/CD pipelines reduce errors and speed up releases Blue/Green and Canary deployments lower deployment risks IaC is the foundation of modern DevOps Infrastructure must be defined as code for consistency, auditing, and automation CDK accelerates development through reusable and maintainable constructs Containerization enables scalable microservices ECS/EKS are well-suited for large and complex systems App Runner is ideal for teams prioritizing simplicity and fast deployment Observability is essential CloudWatch + X-Ray provide complete system visibility Proper alerting significantly reduces MTTR Applying to Work Implement CodePipeline for automated build–test–deploy workflows Use Blue/Green deployments to minimize downtime Adopt CloudFormation for consistent and auditable infrastructure Create CloudWatch dashboards to monitor service performance Deploy microservices with ECS integrated with CodePipeline for full automation Event Experience Attending the “DevOps on AWS” event offered valuable real-world insights into applying DevOps in a cloud-native environment.\nDeep-dive sessions Hands-on demos for CI/CD and IaC Clear understanding of the differences among ECS, EKS, and App Runner Lessons Learned DevOps = culture + processes + tools IaC and CI/CD are the core pillars Observability must be implemented from the beginning Containers are the future of application deployment Overall, the event was highly informative, practical, and provided clear direction for advancing a professional DevOps career.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Strengthen understanding of core AWS services including DynamoDB, CloudFront, and Amazon WorkSpaces through hands-on practice. Learn to deploy, manage, and integrate cloud-based solutions for better performance. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about DynamoDB - Advantages of dynamodb over other databases - Practice: + Create and manage tables in AWS management console + Used AWS CloudShell to perform CRUD operations 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon WorkSpaces - Study the concept and use cases of virtual desktops on AWS - Practice: + Set up the lab environment and deploy a WorkSpace + Remotely access and test the WorkSpace environment 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Set up CloudFront with S3 as the origin - Deployed a CloudFront distribution connected to an S3 bucket 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS CloudFront: + Learn how CloudFront improves content delivery and reduces latency + Creat CloudFront distributions and configured origin groups + Deploy a Lambda@Edge function for dynamic content customization + \u0026hellip; 10/02/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Amazon Nova Canvas (Virtual Try-On) - Write sample code to generate AI-powered virtual try-on images 10/04/2025 10/04/2025 https://aws.amazon.com/vi/blogs/aws/amazon-nova-canvas-update-virtual-try-on-and-style-options-now-available/ Week 4 Achievements: Enhanced Knowledge of NoSQL Databases:\nGained a solid understanding of DynamoDB’s architecture, including tables and partition keys Completed practical exercises to manage data efficiently using both Console and CloudShell Successfully Deployed Amazon WorkSpaces\nConfigured IAM permissions and network settings Deployed a cloud-based virtual desktop and connected to it remotely Implemented CloudFront for global content delivery:\nCreated and configured CloudFront distributions for high-speed, secure content delivery Built an origin group Deployed Lambda@Edge functions \u0026hellip; Integrated CloudFront with S3: configured S3 as an origin source for static content distribution\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: Vietnam Cloud Day 2025\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand AWS organizations, IAM roles, and permission boundaries.\nGain hands-on experience with KMS, cloudtrail and analyzing logs with athena.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS organizations and multi-account strategy - Understand the purpose of core accounts - Practice: + Create AWS accounts + Set up organizational units + Apply customer managed policies + Invite AWS accounts into the organization 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about IAM permission boundary - Learn about IAM role and how to write IAM policies - Practice: + Create a restriction policy and test permission boundaries + Create IAM roles + Write and attach JSON-based policies + Test role assumptions and verify permissions 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn how to manage EC2 access using resource tags - Learn about AWS security hub - Practice: + Enable security hub + Review security standards 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS key management service - Understand symmetric vs asymmetric keys and their use cases - Learn about encrypting data in Amazon S3 using KMS keys - \u0026hellip; 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create AWS cloudtrail to record account activities + Enable logging and store logs in Amazon S3 + Create Amazon athena to analyze cloudtrail logs + Retrieve and query data from athena 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Explored AWS organizations and multi-account strategy:\nBenefits of multi-account Purposes of core accounts Organizational units Customer managed policies \u0026hellip; Became familiar with IAM roles and permission boundaries\nPracticed managing EC2 access with resource tags\nBecame familiar with writing IAM policies using JSON\nConfigured and enabled AWS security hub\nAcquired hands-on experience with AWS key management service:\nUnderstood symmetric vs asymmetric encryption keys\nLearned how KMS integrates with other AWS services\nUsed KMS keys to encrypt and protect data in Amazon S3\nImplemented cloudtrail logging and athena analysis for audit insights\nAcquired the ability to monitor and audit AWS activities effectively\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand advanced AWS services for data migration, security, and automation. Learn how to secure, monitor, and manage AWS resources using GuardDuty, macie, and secrets manager. Gain hands-on experience with lambda and EventBridge for automated EC2 management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS data migration service - Practice: + Setup lab environment + Create a replication instance + Configure source and target endpoints + Create and run a migration task 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon macie\n- Practice: + Create custom data identifiers + Create and run a macie job to analyze sensitive data 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS secrets manager - Practice: + Store RDS credential values securely through AWS secrets manager + Write a bash script to connect to the RDS instance using stored secrets 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon GuardDuty - Learn how to use EventBridge and lambda to automatically isolate compromised EC2 instances when suspicious activity is detected 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about AWS lambda - Practice: + Create a lambda function to start an EC2 instance through EventBridge + Create a Lambda function to stop an EC2 instance through EventBridge 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Got familiar with AWS data migration service and learned how to:\nCreate a replication instance Configure source and target endpoints Create and run migration tasks Being able to detect and classify sensitive data through Amazon macie\nUnderstood how to use AWS secrets manager and practiced:\nStoring RDS credentials securely using secrets manager Writing a bash script to connect to RDS Got familiar with Amazon GuardDuty and enhance security using EventBridge and lambda to automatically protect the system\nLearned how to use AWS lambda and practiced:\nCreate a lambda function to start EC2 instances via EventBridge Create a lambda function to stop EC2 instances via EventBridge. \u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality âœ… â˜ â˜ 2 Ability to learn Ability to absorb new knowledge and learn quickly â˜ âœ… â˜ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions âœ… â˜ â˜ 4 Sense of responsibility Completing tasks on time and ensuring quality âœ… â˜ â˜ 5 Discipline Adhering to schedules, rules, and work processes â˜ â˜ âœ… 6 Progressive mindset Willingness to receive feedback and improve oneself â˜ âœ… â˜ 7 Communication Presenting ideas and reporting work clearly â˜ âœ… â˜ 8 Teamwork Working effectively with colleagues and participating in teams âœ… â˜ â˜ 9 Professional conduct Respecting colleagues, partners, and the work environment âœ… â˜ â˜ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity â˜ âœ… â˜ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team âœ… â˜ â˜ 12 Overall General evaluation of the entire internship period âœ… â˜ â˜ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Visualize and monitor system metrics using grafana Manage and automate AWS operations with systems manager Implement infrastructure as code using CloudFormation and Terraform Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Grafana fundamentals - Practice: + Install grafana + Visualize metrics in dashborads 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn how systems manager centralizes operational tasks\n- Practice: + Use patch manager to automate OS and software patching + Execute remote commands on multiple EC2 instances using run command 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - Understand YAML syntax and the structure of an AWS CloudFormation template - Write a basic template to provision an EC2 instance 10/22/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Terraform fundamentals for automating AWS infrastructure provisioning - Practice: + Write a main.tf file to deploy basic AWS resources + Use variables to protect sensitive data at runtime + Create multiple workspaces to separate environments 10/23/2025 10/26/2025 https://youtu.be/7xngnjfIlK4?si=VhwAI1v4UCCSJQZF 6 - Learn about the basics of continuous integration and deployment\n- Practice: + Use gitHub actions to automate the build and deployment process 10/26/2025 10/26/2025 https://www.youtube.com/watch?v=ZKaDy0mNHGs Week 7 Achievements: Got familiar with grafana fundamentals and core features and know how to:\nInstalled and configured grafana server Created dashboards to visualize system metrics in real time Understood how systems manager centralizes operational management on AWS\nUnderstood the concept of Infrastructure as Code(Terraform and CloudFormation):\nLearned YAML syntax and CloudFormation template structure Created a simple CloudFormation template to provision an EC2 instance Understood key components: parameters, resources, and outputs Studied Terraform basics for automating AWS infrastructure deployment Wrote a main.tf file to launch core AWS resources Used variables to manage sensitive data securely during runtime \u0026hellip; Gained understanding of continuous integration and deployment concepts\nBuilt an automated workflow using gitHub actions for build and deploy processes\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I donâ€™t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Reinforce AWS architecture and core services Learn about Docker Build CI/CD pipelines using Docker and gitHub actions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review AWS architecture + Study fundamental AWS architecture concepts + Understand AWS security architecture best practices + Learn high availability and high-performance architecture design + Explore cost optimization strategies - Review AWS core services : EC2, VPC, IAM, S3, RDS, CloudWatch, etc 10/27/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Docker - Practice: + Use essential Docker commands + Run a simple web application + Write a Dockerfile to build custom images 10/28/2025 10/28/2025 https://www.youtube.com/watch?v=Y3zqsFpUzMk\u0026list=PLncHg6Kn2JT4kLKJ_7uy0x4AdNrCHbe0n 4 - Learn about CI/CD with Docker and gitHub actions - Practice: + Write a gitHub actions workflow to build and test code when pushed + Write and optimize Dockerfile for CI/CD pipelines - Configure gitHub actions to automatically: + Build the Docker image + Deploy the application when new code is pushed 10/29/2025 10/29/2025 https://www.youtube.com/watch?v=4FWrmuKQq1s 5 - Work with Amazon ECR and application deployment - Practice: + Create an EC2 instance to run Docker containers + Create and configure an RDS instance for database usage + Build Docker image and run container locally before deployment + Create an ECR repository and push Docker images to ECR 10/30/2025 10/30/2025 https://youtu.be/7xngnjfIlK4?si=VhwAI1v4UCCSJQZF 6 - Learn about Amazon ECS - Practice: + Create an ECS cluster + Create a task definition based on the image stored in ECR + Deploy the application using ECS service + Test and verify successful deployment 10/31/2025 10/31/2025 https://www.youtube.com/watch?v=ZKaDy0mNHGs Week 8 Achievements: Reinforced AWS Core architecture and services\nThe fundamental cloud architecture model Reviewed core AWS services such as EC2, S3, IAM, RDS, and CloudWatch Identified the role of each service in building scalable applications Got familiar with Docker usage\nPracticed common Docker commands (build, run, ps, logs, exec) Learned how to containerize a simple web application using Docker Wrote a basic Dockerfile and understood image layer structure Got Hands-On with CI/CD using Docker and gitHub actions\nSet up a GitHub Actions workflow to automate application building Configured GitHub Actions pipeline to build Docker images automatically Reviewed how CI/CD helps deploy updated versions on every code push Get Experience Working With Amazon ECR and Deployment\nLaunched and configured an EC2 instance to run Docker containers Deployed a web application container and tested successful operation Created an ECR repository and pushed Docker images to it Understood how ECS Cluster organizes computing resources\nLearned the purpose of task definitions and how they describe container settings\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Deploy containers on ECS using EC2 and Fargate Understand ECS networking modes Implement load balancing and path-based routing with ALB Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice running containers on ECS with EC2: + Create an ECR repository + Build and upload the container image to ECR + Create an ECS cluster + Define a task definition using EC2 + Create a service to launch the task + Verify the deployment is successful 11/03/2025 11/03/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 3 - Practice running containers on ECS with Fargate: + Push the container image to Docker Hub + Create a task definition for Fargate + Create a service and launch the task + Scale the service by increasing the desired task count + Verify the deployment is successful 11/04/2025 11/04/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 4 - Learn about ECS network modes - Understand and compare the following network modes: + bridge + host + awsvpc 11/05/2025 11/05/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 5 - Deploy containers with Application Load Balancer: + Create an Application Load Balancer + Create a target group + Create a service integrated with the ALB + Test and verify that traffic is routed correctly 11/06/2025 11/06/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 6 - Run multiple services behind the same ALB: + Create separate target groups for different paths (/email, /user) + Create services mapped to each target group + Test the deployment to ensure correct path routing 11/07/2025 11/07/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 Week 9 Achievements: Practiced running containerized applications on Amazon ECS:\nBuilt and pushed container images to both ECR and Docker Hub Created ECS clusters using both EC2 and Fargate launch types Created task definitions and deployed services Scaled services by adjusting the desired task count Verified successful application deployment and accessibility Learned about ECS network modes:\nCompared the characteristics and use cases of bridge, host, and awsvpc modes Understood how awsvpc mode assigns ENIs and improves service isolation Gained hands-on experience deploying containers with Application Load Balancer (ALB)\nCreated an ALB and configured target groups Integrated ECS services with the ALB Tested routing and confirmed traffic forwarding worked correctly Deployed multiple services behind a single ALB\nCreated multiple target groups for different application paths (/email, /user, /) Mapped ECS services to the corresponding target groups Tested and verified correct path-based routing behavior "
},
{
	"uri": "http://localhost:1313/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Reinforce AWS DevOps workflow using CodeBuild, CodePipeline, ECR, and ECS Practice building and deploying APIs using API Gateway and Lambda Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Use CodeBuild to automatically push image to ECR + Create a CodeBuild project and connect it to GitHub + Create the required IAM roles for CodeBuild + Write a buildspec.yml file to build and push a Docker image to Amazon ECR + Run a test build to verify that the image is successfully pushed to ECR 11/10/2025 11/10/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 3 - Build a CI/CD Pipeline Using AWS CodePipeline - Practice: + Create a CodePipeline pipeline that listens to GitHub pushes + Create required IAM roles for CodePipeline and deployment actions + Deploy container updates to ECS automatically + Push code to GitHub and verify that the ECS service updates successfully 11/11/2025 11/11/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 4 - Learn about Amazon API Gateway + Create API resources + Create and integrate Lambda functions with API Gateway + Create methods and test invoking the Lambda function 11/12/2025 11/12/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 5 - Practice Input Validation in API Gateway - Practice: + Create API Gateway models to validate request headers and body + Add mapping templates for the method request + Test validation logic using the API Gateway console 11/13/2025 11/13/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 6 - Use stage variables for multi-environment deployment - Practice: + Create a Lambda function and versions + Create aliases and map them to different stages + Add stage variables and reference them in Lambda integration + Test and verify successful deployment 11/14/2025 11/14/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m70lB1qL6sJ9Xr43PVbua3u2 Week 10 Achievements: Strengthened AWS DevOps practices with CodeBuild, CodePipeline, ECR, and ECS\nCreated a CodeBuild project Wrote and tested a complete buildspec.yml to build and push images to Amazon ECR Set up a CodePipeline pipeline triggered on GitHub pushes Integrated source, build, deploy stages to automate container updates on ECS Tested end-to-end deployment and confirmed ECS service updates after each push Got familiar with API Gateway architecture\nCreated resources, methods, and Lambda integrations Built a simple backend API using API Gateway and Lambda Tested method responses and Lambda invocation Implemented request validation in API Gateway\nCreated request models to validate headers and request body Added and configured mapping templates for method requests Verified validation logic using the API Gateway console Practiced multi-environment deployment using stage variables\nCreated Lambda versions and aliases for different environments Configured stage variables to dynamically select Lambda versions Tested multi-stage behavior for dev, staging, and prod "
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]